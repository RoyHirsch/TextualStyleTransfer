{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"E2E_Transformer_adv_classifier.ipynb","version":"0.3.2","provenance":[{"file_id":"1I1TPMLf-A0pijGxuggSGPEIcKeZAnS8g","timestamp":1567505243775},{"file_id":"1KN7tPGYpRbpc4aGlXLdx-EBh_0ER2Aqn","timestamp":1566725697678},{"file_id":"1T0enE9NtODXGHcr2VneOw4xAZGzyhTSA","timestamp":1566397151425},{"file_id":"1kz3LqI-Lso2VKWPg2Au497TaaLqOH3Eu","timestamp":1564756850569},{"file_id":"1pVmZg0SxJcqtbKigz4EL-26vlHKNyIO5","timestamp":1564483102744}],"collapsed_sections":["8yMShB4YHl2s"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SYt0p9QnQksE","colab_type":"text"},"source":["A script to train an encoder-decoder based on transformer for sytle transfer.\n","\n","Based on Harvard implementation of Transformer: http://nlp.seas.harvard.edu/2018/04/03/attention.html\n","\n","Also based on: https://arxiv.org/pdf/1711.06861.pdf"]},{"cell_type":"markdown","metadata":{"id":"wMnYQvWidp7W","colab_type":"text"},"source":["# Drive"]},{"cell_type":"code","metadata":{"id":"JBeBJOezdup5","colab_type":"code","outputId":"d14ab7d9-e0e6-475e-97ae-a6022098dc73","executionInfo":{"status":"ok","timestamp":1568317072968,"user_tz":-180,"elapsed":24108,"user":{"displayName":"Gal Shachaf","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAmvlpUu2oQV5trCyjFvlOX1Yp8tPVeVSHp3KeehEc=s64","userId":"09922006514276555207"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iRyVnkrXxu3s","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","CODE_PATH = '/content/drive/My Drive/NLP/final/TextualStyleTransfer/'\n","sys.path.append(CODE_PATH)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QYqsE9HI_d_F","colab_type":"text"},"source":["# Init"]},{"cell_type":"code","metadata":{"id":"eb9tT04Z-6-3","colab_type":"code","outputId":"5b8c9cb7-32a8-47d4-cb57-412272c5deb1","executionInfo":{"status":"ok","timestamp":1568317097659,"user_tz":-180,"elapsed":48753,"user":{"displayName":"Gal Shachaf","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAmvlpUu2oQV5trCyjFvlOX1Yp8tPVeVSHp3KeehEc=s64","userId":"09922006514276555207"}},"colab":{"base_uri":"https://localhost:8080/","height":833}},"source":["!pip install pytorch-transformers\n","!pip install torchtext"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting pytorch-transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n","\r\u001b[K     |█▉                              | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.16.5)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.9.224)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n","Collecting regex (from pytorch-transformers)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n","\u001b[K     |████████████████████████████████| 655kB 46.9MB/s \n","\u001b[?25hCollecting sentencepiece (from pytorch-transformers)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 43.2MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n","Collecting sacremoses (from pytorch-transformers)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/24/0b86f494d3a5c7531f6d0c77d39fd8f9d42e651244505d3d737e31db9a4d/sacremoses-0.0.33.tar.gz (802kB)\n","\u001b[K     |████████████████████████████████| 808kB 37.8MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.1.0)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.2.1)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.224 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.12.224)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.6.16)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.13.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->pytorch-transformers) (2.5.3)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->pytorch-transformers) (0.15.2)\n","Building wheels for collected packages: regex, sacremoses\n","  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609237 sha256=b9485ccaa9fb3edec2a9fd39634378ca0bb13b8eee2fca5dfa18615bb22f4f7b\n","  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.33-cp36-none-any.whl size=833106 sha256=6d16e02b37e6fb37dc7edcf16398b481bf6aee47348a0cd9416420a3fb215c51\n","  Stored in directory: /root/.cache/pip/wheels/70/87/56/e40575cca30d12fee8875d523b8878b7aba866a9f03b2fd983\n","Successfully built regex sacremoses\n","Installing collected packages: regex, sentencepiece, sacremoses, pytorch-transformers\n","Successfully installed pytorch-transformers-1.2.0 regex-2019.8.19 sacremoses-0.0.33 sentencepiece-0.1.83\n","Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.16.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.6.16)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q1nuINMLINeC","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","import numpy as np\n","import logging\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset, RandomSampler\n","\n","from data import *\n","from train import *\n","from evaluate import *\n","from utils import *"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bis4Ep1DNoof","colab_type":"text"},"source":["# Parameters"]},{"cell_type":"code","metadata":{"id":"Nn4iWNlVNqoT","colab_type":"code","outputId":"34e7053e-eafe-49db-b5c6-fb5bd597e05e","executionInfo":{"status":"ok","timestamp":1568317107488,"user_tz":-180,"elapsed":57030,"user":{"displayName":"Gal Shachaf","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAmvlpUu2oQV5trCyjFvlOX1Yp8tPVeVSHp3KeehEc=s64","userId":"09922006514276555207"}},"colab":{"base_uri":"https://localhost:8080/","height":833}},"source":["class Params(object):\n","\n","    # Loggin\n","    # Free text to describe the experiment\n","    COMMENT = ''\n","    VERBOSE = True\n","    EXP_NAME = \"gal_exp\"\n","    DATA_PATH = \"/content/drive/My Drive/NLP/\"\n","    MODELS_PATH = \"/content/drive/My Drive/NLP/final/\"\n","    PRINT_INTERVAL = 10\n","\n","    # TODO: for local use\n","    # DATA_PATH = MODELS_PATH = os.path.abspath(__file__+'/../')\n","    MODELS_LOAD_PATH = \"/content/drive/My Drive/NLP/final/gal_exp\"\n","\n","    # Data\n","    DATASET_NAME = 'IMDB'\n","    # Maximal number of batches for test model\n","    TEST_MAX_BATCH_SIZE = 300\n","    # Min freq for word in dataset to include in vocab\n","    VOCAB_MIN_FREQ = 3\n","    # Whether to use Glove embadding - if TRUE set H_DIM to 300\n","    VOCAB_USE_GLOVE = True\n","    TRAIN_BATCH_SIZE = 32\n","    TEST_BATCH_SIZE = 32\n","    # maximum length of allowed sentence - can be also None\n","    MAX_LEN = 25\n","\n","    # Transformer model\n","    N_LAYERS = 8\n","    N_LAYERS_CLS = 4\n","    H_DIM = 300\n","    N_ATTN_HEAD = 5\n","    FC_DIM = 2048\n","    DO_RATE = 0.1\n","\n","    # Classification model\n","    N_STYLES = 2\n","    DO_RATE_CLS = 0.1\n","    TRANS_CLS = True\n","    TRANS_DES = True\n","    CLS_ACC_BAR = 95.0\n","    NEG_CLS_ACC_BAR = 95.0\n","\n","    # Train5\n","    N_EPOCHS = 20\n","    PATIENCE = 3\n","    ENC_LR = 0\n","    DEC_LR = 3e-4\n","    CLS_LR = 3e-4\n","    TRANS_STEPS_RATIO = 0.1\n","    TRUE_STEPS_RATIO = 0.9\n","    PERIOD_STEPS = 100\n","    ENC_WARMUP_RATIO = 0.1\n","    DEC_WARMUP_RATIO = 0.2\n","    CLS_WARMUP_RATIO = 0.2\n","    TRUE_REC_LAMBDA = 1e-2\n","    TRUE_CLS_LAMBDA = 0.0\n","    NEG_CYC_REC_LAMBDA = 0.5\n","    NEG_REC_LAMBDA = 0.0\n","    NEG_DES_LAMBDA = 0.0\n","    NEG_CLS_LAMBDA = 0.5\n","    TRAIN_ON_CLS_LOSS = False\n","    REC_ACC_BAR = 80.0\n","    DES_ACC_BAR = 60.0\n","\n","\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","params = Params()\n","logger = create_logger(params)\n","pprint_params(params)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["INFO - 09/12/19 19:38:26 - 0:00:00 - Params for experiment:\n","INFO - 09/12/19 19:38:26 - 0:00:01 - CLS_ACC_BAR = 95.0\n","INFO - 09/12/19 19:38:26 - 0:00:01 - CLS_LR = 0.0003\n","INFO - 09/12/19 19:38:26 - 0:00:01 - CLS_WARMUP_RATIO = 0.2\n","INFO - 09/12/19 19:38:26 - 0:00:01 - COMMENT = ''\n","INFO - 09/12/19 19:38:26 - 0:00:01 - DATASET_NAME = 'IMDB'\n","INFO - 09/12/19 19:38:26 - 0:00:01 - DATA_PATH = '/content/drive/My Drive/NLP/'\n","INFO - 09/12/19 19:38:26 - 0:00:01 - DEC_LR = 0.0003\n","INFO - 09/12/19 19:38:26 - 0:00:01 - DEC_WARMUP_RATIO = 0.2\n","INFO - 09/12/19 19:38:26 - 0:00:01 - DES_ACC_BAR = 60.0\n","INFO - 09/12/19 19:38:26 - 0:00:01 - DO_RATE = 0.1\n","INFO - 09/12/19 19:38:26 - 0:00:01 - DO_RATE_CLS = 0.1\n","INFO - 09/12/19 19:38:26 - 0:00:01 - ENC_LR = 0\n","INFO - 09/12/19 19:38:26 - 0:00:01 - ENC_WARMUP_RATIO = 0.1\n","INFO - 09/12/19 19:38:26 - 0:00:01 - EXP_NAME = 'gal_exp'\n","INFO - 09/12/19 19:38:26 - 0:00:01 - FC_DIM = 2048\n","INFO - 09/12/19 19:38:26 - 0:00:01 - H_DIM = 300\n","INFO - 09/12/19 19:38:26 - 0:00:01 - MAX_LEN = 25\n","INFO - 09/12/19 19:38:26 - 0:00:01 - MODELS_LOAD_PATH = '/content/drive/My Drive/NLP/final/gal_exp'\n","INFO - 09/12/19 19:38:26 - 0:00:01 - MODELS_PATH = '/content/drive/My Drive/NLP/final/'\n","INFO - 09/12/19 19:38:26 - 0:00:01 - NEG_CLS_ACC_BAR = 95.0\n","INFO - 09/12/19 19:38:26 - 0:00:01 - NEG_CLS_LAMBDA = 0.5\n","INFO - 09/12/19 19:38:26 - 0:00:01 - NEG_CYC_REC_LAMBDA = 0.5\n","INFO - 09/12/19 19:38:26 - 0:00:01 - NEG_DES_LAMBDA = 0.0\n","INFO - 09/12/19 19:38:26 - 0:00:01 - NEG_REC_LAMBDA = 0.0\n","INFO - 09/12/19 19:38:26 - 0:00:01 - N_ATTN_HEAD = 5\n","INFO - 09/12/19 19:38:26 - 0:00:01 - N_EPOCHS = 20\n","INFO - 09/12/19 19:38:26 - 0:00:01 - N_LAYERS = 8\n","INFO - 09/12/19 19:38:26 - 0:00:01 - N_LAYERS_CLS = 4\n","INFO - 09/12/19 19:38:26 - 0:00:01 - N_STYLES = 2\n","INFO - 09/12/19 19:38:26 - 0:00:01 - PATIENCE = 3\n","INFO - 09/12/19 19:38:26 - 0:00:01 - PERIOD_STEPS = 100\n","INFO - 09/12/19 19:38:26 - 0:00:01 - PRINT_INTERVAL = 10\n","INFO - 09/12/19 19:38:26 - 0:00:01 - REC_ACC_BAR = 80.0\n","INFO - 09/12/19 19:38:26 - 0:00:01 - TEST_BATCH_SIZE = 32\n","INFO - 09/12/19 19:38:26 - 0:00:01 - TEST_MAX_BATCH_SIZE = 300\n","INFO - 09/12/19 19:38:26 - 0:00:01 - TRAIN_BATCH_SIZE = 32\n","INFO - 09/12/19 19:38:26 - 0:00:01 - TRAIN_ON_CLS_LOSS = False\n","INFO - 09/12/19 19:38:26 - 0:00:01 - TRANS_CLS = True\n","INFO - 09/12/19 19:38:26 - 0:00:01 - TRANS_DES = True\n","INFO - 09/12/19 19:38:26 - 0:00:01 - TRANS_STEPS_RATIO = 0.1\n","INFO - 09/12/19 19:38:26 - 0:00:01 - TRUE_CLS_LAMBDA = 0.0\n","INFO - 09/12/19 19:38:26 - 0:00:01 - TRUE_REC_LAMBDA = 0.01\n","INFO - 09/12/19 19:38:26 - 0:00:01 - TRUE_STEPS_RATIO = 0.9\n","INFO - 09/12/19 19:38:26 - 0:00:01 - VERBOSE = True\n","INFO - 09/12/19 19:38:26 - 0:00:01 - VOCAB_MIN_FREQ = 3\n","INFO - 09/12/19 19:38:26 - 0:00:01 - VOCAB_USE_GLOVE = True\n","INFO - 09/12/19 19:38:26 - 0:00:01 - device = device(type='cuda', index=0)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ULHsog2w8vEu","colab_type":"code","outputId":"860d5042-e25f-4f6b-9681-af591144374f","executionInfo":{"status":"ok","timestamp":1568042774427,"user_tz":-180,"elapsed":2985,"user":{"displayName":"Gal Shachaf","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAmvlpUu2oQV5trCyjFvlOX1Yp8tPVeVSHp3KeehEc=s64","userId":"09922006514276555207"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# import pandas as pd\n","\n","# def create_mini_csv(csv_path, new_csv_path, size):\n","#     data = pd.read_csv(csv_path)\n","#     data = data.loc[:size]\n","#     print(len(data))\n","#     data.to_csv(new_csv_path.format(size))\n","\n","# create_mini_csv(\"/content/drive/My Drive/NLP/YELP/yelp_train.csv\", \"/content/drive/My Drive/NLP/YELP/yelp_train_{}.csv\", 100000)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100001\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qwdhTRiylqRz","colab_type":"text"},"source":["# Data"]},{"cell_type":"code","metadata":{"id":"5LNaz4_Mwaio","colab_type":"code","colab":{}},"source":["# import torchtext\n","# from torchtext.data import Field, LabelField, TabularDataset\n","# from spacy.lang.en import English\n","# import os\n","\n","# TEXT, word_embeddings, train_iter, test_iter = load_dataset_from_csv(params=params, device=params.device)\n","# train_dataset_len = len(train_iter.dataset)\n","# print('Train dataset len: {} Test dataset len: {}'.format(len(train_iter.dataset), len(test_iter.dataset)))\n","\n","import torchtext\n","from torchtext.data import Field, LabelField, TabularDataset\n","from spacy.lang.en import English\n","\n","en = English()\n","\n","def tokenize(sentence):\n","    return [tok.text for tok in en.tokenizer(sentence)]\n","\n","TEXT = Field(sequential=True, tokenize=tokenize, lower=True, eos_token='<eos>', batch_first=True, fix_length=params.MAX_LEN)\n","LABEL = LabelField()\n","\n","fields_list = [('Unnamed: 0', None),\n","                ('text', TEXT),\n","                ('label', LABEL)]\n","\n","train_dataset = TabularDataset(\n","                            path=\"/content/drive/My Drive/NLP/YELP/yelp_train.csv\", # the root directory where the data lies\n","                            format='csv',\n","                            skip_header=True, \n","                            fields=fields_list)\n","\n","test_dataset = TabularDataset(\n","                            path=\"/content/drive/My Drive/NLP/YELP/yelp_test_200.csv\", # the root directory where the data lies\n","                            format='csv',\n","                            skip_header=True, \n","                            fields=fields_list)\n","\n","if params.VOCAB_USE_GLOVE:\n","    TEXT.build_vocab(train_dataset, test_dataset, min_freq=params.VOCAB_MIN_FREQ, vectors=GloVe(name='6B', dim=params.H_DIM))\n","    logging.info(\"Loaded Glove embedding, Vector size of Text Vocabulary: \" + str(TEXT.vocab.vectors.size()))\n","\n","else:\n","    TEXT.build_vocab(train_dataset, test_dataset, min_freq=params.VOCAB_MIN_FREQ)\n","LABEL.build_vocab(train_dataset)\n","\n","word_embeddings = TEXT.vocab.vectors\n","logging.info(\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n","\n","train_iter, test_iter = data.BucketIterator.splits((train_dataset, test_dataset),\n","                                                    batch_sizes=(params.TRAIN_BATCH_SIZE, params.TRAIN_BATCH_SIZE),\n","                                                    sort_key=lambda x: len(x.text), repeat=False, shuffle=True,\n","                                                    device=params.device)\n","# Disable shuffle\n","test_iter.shuffle = False\n","\n","train_dataset_len = len(train_iter.dataset)\n","print('Train dataset len: {} Test dataset len: {}'.format(len(train_iter.dataset), len(test_iter.dataset)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6gq_qV7WSKEA","colab_type":"code","colab":{}},"source":["from transformer_model import *\n","import torch.nn.functional as F\n","\n","class ArgMaxEmbed(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, inputs, embed, sample):\n","        if sample:\n","            m = torch.distributions.Categorical(logits=inputs)\n","            idx = m.sample()\n","        else:\n","            idx = torch.argmax(inputs, -1)\n","        ctx._input_shape = inputs.shape\n","        ctx._input_dtype = inputs.dtype\n","        ctx._input_device = inputs.device\n","        ctx.save_for_backward(idx)\n","        return embed(idx)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        idx, = ctx.saved_tensors\n","        grad_input = torch.zeros(ctx._input_shape, device=ctx._input_device, dtype=ctx._input_dtype)\n","        # print(\"backward debug\", idx[..., None].shape, grad_output.sum(-1, keepdim=True), grad_input.shape)\n","        grad_input.scatter_(-1, idx[..., None], grad_output.sum(-1, keepdim=True))\n","        return grad_input, None, None\n","\n","class StyleTransformer(nn.Module):\n","    \"\"\"\n","    An encoder that also encodes style and adds it to the representation\n","    \"\"\"\n","    def __init__(self, src_vocab, tgt_vocab, N=6,\n","                    d_model=512, d_ff=2048, h=8, n_styles=2, dropout=0.1, max_len=128):\n","        super().__init__()\n","        c = copy.deepcopy\n","        attn = MultiHeadedAttention(h, d_model)\n","        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n","\n","        self.src_embed = Embeddings(d_model, src_vocab)\n","        self.argmax = ArgMaxEmbed.apply\n","        self.encoder = BasicEncoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)\n","        self.position = PositionalEncoding(d_model, dropout, max_len)\n","        self.style_embed = nn.Embedding(n_styles, d_model)\n","        self.generator = nn.Linear(d_model, tgt_vocab)\n","\n","        # Initialize parameters with Glorot / fan_avg.\n","        for p in self.parameters():\n","            if p.dim() > 1:\n","                nn.init.xavier_uniform_(p)\n","\n","    def encode_style(self, style_labels):\n","        style_embadding = self.style_embed(style_labels).unsqueeze(1)\n","        if style_embadding.ndimension() == 1:\n","            style_embadding = style_embadding.unsqueeze(0).unsqueeze(1)\n","        elif style_embadding.ndimension() == 2:\n","            style_embadding = style_embadding.permute(1, 0).unsqueeze(0)\n","        return style_embadding\n","\n","    def forward(self, src, src_mask, style, argmax=False):\n","        \"Take in and process masked src and target sequences.\"\n","        style = self.style_embed(style).unsqueeze(dim=1)\n","        if argmax:\n","            src = self.argmax(src, self.src_embed, False)\n","        else:\n","            src = self.src_embed(src)\n","        src = self.position(src)\n","        # add style before position?\n","        x = src + style\n","        enc_out = self.encoder(x, src_mask)\n","        return self.generator(enc_out)\n","\n","class MaskedMean(nn.Module):\n","    \" Calculate masked mean of input 3D tensor \"\n","\n","    def __init__(self, normalize=True):\n","        self.normalize = normalize\n","        super().__init__()\n","\n","    def forward(self, x, mask):\n","        batch_size, _, embed_size = x.size()\n","        mask_expanded = mask.transpose(-1, -2).expand(-1, -1, embed_size).float()\n","        masked_input = x * mask_expanded\n","        sum_ = torch.sum(masked_input, 1)\n","        div = mask.sum(-1).float()\n","        embed = torch.div(sum_, div.view(batch_size, 1))\n","        if self.normalize:\n","            return F.normalize(embed, p=2, dim=1)\n","        else:\n","            return embed\n","\n","class TransformerClassifier(nn.Module):\n","    \"\"\"\n","    Transformer for style classification\n","    \"\"\"\n","    def __init__(self, output_size, input_size, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1, max_len=128):\n","        super().__init__()\n","        # self.src_embed = nn.Linear(input_size, d_model)\n","        self.src_embed = Embeddings(d_model, input_size)\n","        self.argmax = ArgMaxEmbed.apply\n","        c = copy.deepcopy\n","        attn = MultiHeadedAttention(h, d_model)\n","        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n","        self.position = PositionalEncoding(d_model, dropout, max_len)\n","        self.encoder = BasicEncoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)\n","        self.generator = nn.Linear(d_model, output_size)\n","        self.masked_mean = MaskedMean()\n","\n","        for p in self.parameters():\n","            if p.dim() > 1:\n","                nn.init.xavier_uniform_(p)\n","\n","    def forward(self, src, src_mask, argmax=False):\n","        if argmax:\n","            src = self.argmax(src, self.src_embed, False)\n","        else:\n","            src = self.src_embed(src)\n","        src = self.position(src)\n","        out = self.encoder(src, src_mask)\n","        out = self.generator(out)\n","        out = self.masked_mean(out, src_mask)\n","        return out\n","\n","\n","def init_models(vocab_size, params):\n","    model_trans = StyleTransformer(src_vocab=vocab_size, tgt_vocab=vocab_size,\n","                                        N=params.N_LAYERS, d_model=params.H_DIM, d_ff=params.FC_DIM,\n","                                        h=params.N_ATTN_HEAD, n_styles=params.N_STYLES, dropout=params.DO_RATE, max_len=params.MAX_LEN)\n","    if params.TRANS_CLS:\n","        model_cls = TransformerClassifier(output_size=params.N_STYLES, N=params.N_LAYERS_CLS, d_model=params.H_DIM,\n","                                        d_ff=params.FC_DIM, h=params.N_ATTN_HEAD, dropout=params.DO_RATE_CLS,\n","                                        input_size=vocab_size, max_len=params.MAX_LEN)\n","    else:\n","        model_cls = Descriminator(input_size=vocab_size, output_size=params.N_STYLES, hidden_size=params.H_DIM,\n","                            embedding_size=params.H_DIM, drop_rate=params.DO_RATE_CLS, num_layers=params.N_LAYERS_CLS,\n","                            num_layers_for_output=4)\n","    return model_trans, model_cls\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UvJIoQdwM7kZ","colab_type":"text"},"source":["# Models"]},{"cell_type":"code","metadata":{"id":"hxbWN9UvXpeu","colab_type":"code","colab":{}},"source":["def load_pretrained_embedding_to_encoder(src_embed, embedding):\n","    ''' Helper function to modify encoder model embedding with pre-trained\n","        embedding like Glove. '''\n","    src_embed.lut.weight.data.copy_(embedding)\n","    print('Loaded pre-calculated Glove embedding')\n","\n","# Clear CUDA memory if needed\n","# TODO: local use\n","# torch.cuda.empty_cache()\n","\n","### Init models ###\n","\n","vocab_size = len(TEXT.vocab)\n","model_dec, model_cls = init_models(vocab_size, params)\n","# model_des = TransformerClassifier(output_size=params.N_STYLES+1, N=params.N_LAYERS_CLS, d_model=params.H_DIM,\n","#                                 d_ff=params.FC_DIM, h=params.N_ATTN_HEAD, dropout=params.DO_RATE_CLS,\n","#                                 input_size=vocab_size, max_len=params.MAX_LEN)\n","\n","if params.H_DIM == 300:\n","  load_pretrained_embedding_to_encoder(model_dec.src_embed, word_embeddings)\n","  load_pretrained_embedding_to_encoder(model_cls.src_embed, word_embeddings)\n","#   load_pretrained_embedding_to_encoder(model_des.src_embed, word_embeddings)\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'model_cls has {count_parameters(model_cls):,} trainable parameters')\n","print(f'model_dec has {count_parameters(model_dec):,} trainable parameters')\n","# print(f'model_des has {count_parameters(model_des):,} trainable parameters')\n","\n","\n","model_dec = model_dec.to(params.device)\n","model_cls = model_cls.to(params.device)\n","# model_des = model_des.to(params.device)\n","\n","### Init optimizers ###\n","def get_warmup_steps_from_params(train_set_size, train_batch_size, n_epochs,\n","                                 enc_ratio, dec_ratio, cls_ratio):\n","    steps_per_epoch = train_set_size // train_batch_size\n","    n_total_steps = n_epochs * steps_per_epoch\n","    warmup_dec_steps = n_total_steps * dec_ratio\n","    warmup_cls_steps = n_total_steps * cls_ratio\n","\n","    logging.info(\"total_steps {}, dec_warmup {}, cls_warmup {}\".format(n_total_steps, warmup_dec_steps,\n","                                                        warmup_cls_steps))\n","    return warmup_dec_steps, warmup_cls_steps\n","\n","\n","dec_warmup, cls_warmup = get_warmup_steps_from_params(train_dataset_len,\n","                                                                  params.TRAIN_BATCH_SIZE,\n","                                                                  params.N_EPOCHS,\n","                                                                  params.ENC_WARMUP_RATIO,\n","                                                                  params.DEC_WARMUP_RATIO,\n","                                                                  params.CLS_WARMUP_RATIO)\n","des_warmup = dec_warmup\n","# cls_opt = get_std_opt(model_cls,h_dim=params.H_DIM, lr=params.CLS_LR, warmup=cls_warmup)\n","# opt_cls = torch.optim.Adam(filter(lambda p: p.requires_grad, model_cls.parameters()),\n","#                            lr=params.CLS_LR, weight_decay=1e-4)\n","# opt_dec = torch.optim.Adam(filter(lambda p: p.requires_grad, model_cls.parameters()),\n","#                            lr=params.DEC_LR, weight_decay=1e-4)\n","\n","opt_cls = get_std_opt(model_cls,h_dim=params.H_DIM, lr=params.CLS_LR, warmup=4000, factor=1)\n","# opt_des = get_std_opt(model_des,h_dim=params.H_DIM, lr=params.CLS_LR, warmup=des_warmup)\n","\n","\n","# # cls_opt = torch.optim.SGD(model_cls.parameters(), 5e-5)\n","\n","opt_dec = get_std_opt(model_dec,h_dim=params.H_DIM, lr=params.DEC_LR, warmup=3000, factor=0.7)\n","# dec_opt = get_std_opt(model_dec,h_dim=params.H_DIM, lr=params.DEC_LR, warmup=dec_warmup)\n","# cls_dec_opt = get_std_opt(model_cls_dec,h_dim=params.H_DIM, lr=params.DEC_LR, warmup=dec_warmup)\n","\n","# early_stop = EarlyStopping(params.PATIENCE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vcqAGUkk78Tp","colab_type":"code","colab":{}},"source":["### Init losses ###\n","cls_criteria = nn.CrossEntropyLoss()\n","cls_criteria = cls_criteria.to(params.device)\n","des_criteria = nn.CrossEntropyLoss()\n","des_criteria = cls_criteria.to(params.device)\n","\n","seq2seq_criteria = nn.CrossEntropyLoss(reduction='mean', ignore_index=1)\n","# seq2seq_criteria = MaskedCosineEmbeddingLoss(params.device)\n","seq2seq_criteria = seq2seq_criteria.to(params.device)\n","\n","# ent_criteria = EntropyLoss()\n","# ent_criteria = ent_criteria.to(params.device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RSUSqRBxHh42","colab_type":"text"},"source":["# train funcs"]},{"cell_type":"code","metadata":{"id":"yRRFHlB_Ivzc","colab_type":"code","colab":{}},"source":["def train_neg_label_step(model_dec, seq2seq_criteria, model_cls, model_des, cls_criteria, des_criteria,\n","                         opt_dec, src, src_mask, labels, rec_running_loss, rec_acc, cls_running_loss,\n","                         cls_acc, des_running_loss, des_acc, device, trans_cls=False, cyc_rec_lambda=1.0, cls_lambda=1.0, rec_lambda=0.0, des_lambda=0.0):\n","    model_dec.train()\n","    # Negate labels\n","    neg_labels = (~labels.byte()).long()\n","    neg_preds = model_dec(src, src_mask, neg_labels, argmax=False)\n","    # true_preds = model_dec(src, src_mask, labels, argmax=False)\n","    if trans_cls:\n","        cls_preds = model_cls(neg_preds, src_mask, argmax=True)\n","        # des_preds = model_des(neg_preds, src_mask, argmax=True)\n","    else:\n","        cls_preds = model_cls(neg_preds)\n","        # des_preds = model_des(neg_preds)\n","\n","    # neg_sent_mask, _ = make_masks(neg_sent, neg_sent, device)\n","    cyc_preds = model_dec(neg_preds, src_mask, labels, argmax=True)\n","\n","    opt_dec.zero_grad()\n","    # classifier\n","    cls_loss = cls_criteria(cls_preds, neg_labels)\n","    cls_acc.update(cls_preds, neg_labels)\n","    cls_running_loss.update(cls_loss)\n","    # descriminator\n","    # des_labels = torch.ones_like(labels)\n","    # des_labels = neg_labels\n","    # des_loss = des_criteria(des_preds, des_labels)\n","    # des_acc.update(des_preds, des_labels)\n","    # des_running_loss.update(des_loss)\n","    # rec and cycle_rec loss\n","    cyc_preds = cyc_preds.contiguous().view(-1, cyc_preds.size(-1))\n","    # true_preds = true_preds.contiguous().view(-1, true_preds.size(-1))\n","    # neg_preds = neg_preds.contiguous().view(-1, preds.size(-1))\n","    src = src.contiguous().view(-1)\n","    cyc_rec_loss = seq2seq_criteria(cyc_preds, src)\n","    # rec_loss = seq2seq_criteria(true_preds, src)\n","    rec_running_loss.update(cyc_rec_loss)\n","    rec_acc.update(cyc_preds, src)\n","\n","    # optimize\n","    # loss = cyc_rec_lambda * cyc_rec_loss + rec_lambda * rec_loss + des_lambda * des_loss + cls_lambda * cls_loss\n","    # # loss = cyc_rec_lambda * cyc_rec_loss + des_lambda * des_loss\n","    loss = cyc_rec_lambda * cyc_rec_loss + cls_lambda * cls_loss\n","    # loss = cyc_rec_lambda * cyc_rec_loss + rec_lambda * rec_loss + cls_lambda * cls_loss\n","    loss.backward()\n","    # print(\"model_dec last layer grads\", sum(sum(abs(model_dec.generator.weight.grad))))\n","    opt_dec.step()\n","    \n","\n","\n","def train_true_neg_cls_step(model_dec, model_cls, cls_criteria,\n","                            opt_cls, src, src_mask, labels, cls_running_loss,\n","                            cls_acc, trans_cls=False):\n","    # style classifier loss\n","    if trans_cls:\n","        cls_preds = model_cls(src, src_mask, argmax=False)\n","    else:\n","        cls_preds = model_cls(src)\n","\n","    opt_cls.zero_grad()\n","    cls_loss = cls_criteria(cls_preds, labels)\n","    cls_acc.update(cls_preds, labels)\n","    cls_running_loss.update(cls_loss)\n","    cls_loss.backward()\n","    # print(\"cls_encode sum abs grads\", model_cls.src_embed.lut.weight.grad)\n","    opt_cls.step()\n","\n","def train_des_step(model_dec, model_des, opt_des, criteria,\n","                         src, src_mask, labels, des_running_loss,\n","                         des_acc, device, trans_des=False):\n","    model_dec.eval()\n","    # Negate labels\n","    neg_labels = (~labels.byte()).long()\n","\n","    with torch.no_grad():\n","        neg_preds = model_dec(src, src_mask, neg_labels, argmax=False)\n","    if trans_des:\n","        fake_preds = model_des(neg_preds, src_mask, argmax=True)\n","        true_preds = model_des(src, src_mask, argmax=False)\n","    else:\n","        fake_preds = model_des(neg_preds)\n","        true_preds = model_des(src)\n","\n","\n","    # true_labels = torch.ones_like(labels)\n","    true_labels = labels\n","    des_acc.update(true_preds, true_labels)\n","    true_loss = criteria(true_preds, true_labels)\n","\n","    # fake_labels = torch.zeros_like(labels)\n","    fake_labels = 2 * torch.ones_like(labels)\n","    fake_loss = criteria(fake_preds, fake_labels)\n","    des_acc.update(fake_preds, fake_labels)\n","\n","    loss = (true_loss + fake_loss) / 2\n","    des_running_loss.update(loss)\n","\n","    opt_des.zero_grad()\n","    loss.backward()\n","    # print(\"model_dec last layer grads\", sum(sum(abs(model_dec.generator.weight.grad))))\n","    opt_des.step()\n","\n","def run_epoch_true_neg(epoch, data_iter, model_dec, opt_dec,\n","                       model_cls, opt_cls, model_des, opt_des, cls_criteria, des_criteria, seq2seq_criteria,\n","                       params):\n","    verbose = params.VERBOSE\n","    device = params.device\n","    total_steps = len(data_iter.dataset) // params.TRAIN_BATCH_SIZE\n","    period_steps = params.PERIOD_STEPS\n","    logging.info('total epoch steps {}, period size {}'.format(total_steps,\n","                                                               period_steps))\n","\n","    cls_running_loss = Loss()\n","    rec_running_loss = Loss()\n","    des_running_loss = Loss()\n","\n","\n","    cls_acc = AccuracyCls()\n","    des_acc = AccuracyCls()\n","    rec_acc = AccuracyRec()\n","\n","    model_cls.train()\n","    model_dec.train()\n","    # model_des.train()\n","    curr_step = 0\n","    curr_phase = 2\n","    for step, batch in enumerate(data_iter):\n","        # prepare batch\n","\n","        src, labels = batch.text, batch.label\n","\n","        src_mask, _ = make_masks(src, src, device)\n","\n","        src = src.to(device)\n","        src_mask = src_mask.to(device)\n","        labels = labels.to(device)\n","\n","        if curr_phase == 0:  # training the classifier\n","            train_true_neg_cls_step(model_dec, model_cls, cls_criteria,\n","                                    opt_cls, src, src_mask, labels, cls_running_loss,\n","                                    cls_acc, trans_cls=params.TRANS_CLS)\n","            curr_step += 1\n","            if curr_step == period_steps:\n","                if verbose:\n","                    logging.info(\n","                        \"e-{},s-{}: Trained cls loss {:.3f} acc {:.3f}\".format(epoch, step, cls_running_loss(),\n","                                                                        cls_acc()))\n","                cls_running_loss.reset()\n","                curr_step = 0\n","                if cls_acc() >= params.CLS_ACC_BAR:\n","                    curr_phase = 1\n","                cls_acc.reset()\n","\n","        elif curr_phase == 1:\n","            train_des_step(model_dec, model_des, opt_des, des_criteria,\n","                         src, src_mask, labels, des_running_loss,\n","                         des_acc, device, trans_des=params.TRANS_DES)\n","            curr_step += 1\n","            if curr_step == period_steps:\n","                if verbose:\n","                    logging.info(\n","                        \"e-{},s-{}: Trained des, loss {:.3f}, acc {:.3f}\".format(epoch,\n","                                                                                            step,\n","                                                                                            des_running_loss(),\n","                                                                                            des_acc()))\n","                curr_step = 0\n","                if des_acc() >= params.DES_ACC_BAR:\n","                    curr_phase = curr_phase + 1\n","                des_running_loss.reset()\n","                des_acc.reset()\n","\n","        else:\n","            train_neg_label_step(model_dec=model_dec, seq2seq_criteria=seq2seq_criteria, model_cls=model_cls, cls_criteria=cls_criteria, des_criteria=des_criteria, model_des=model_des,\n","                                 des_running_loss=des_running_loss, des_acc=des_acc,\n","                                 opt_dec=opt_dec, src=src, src_mask=src_mask, labels=labels, rec_running_loss=rec_running_loss, rec_acc=rec_acc, cls_running_loss=cls_running_loss,\n","                                 cls_acc=cls_acc, device=params.device, trans_cls=params.TRANS_CLS, cyc_rec_lambda=params.NEG_CYC_REC_LAMBDA,\n","                                 rec_lambda=params.NEG_REC_LAMBDA, cls_lambda=params.NEG_CLS_LAMBDA, des_lambda=params.NEG_DES_LAMBDA)\n","\n","            curr_step += 1\n","            if curr_step == period_steps:\n","                if verbose:\n","                    logging.info(\n","                        \"e-{},s-{}: Trained transformer on negated label, cls_loss {:.3f}, cls_acc {:.3f}, rec_loss {:.3f}, rec_acc {:.3f}, des_loss {:.3f}, des_acc {:.3f}\".format(\n","                            epoch,\n","                            step,\n","                            cls_running_loss(),\n","                            cls_acc(),\n","                            rec_running_loss(),\n","                            rec_acc(),\n","                            des_running_loss(),\n","                            des_acc()))\n","                curr_step = 0\n","                curr_phase = 2\n","                cls_acc.reset()\n","                rec_running_loss.reset()\n","                cls_running_loss.reset()\n","                rec_acc.reset()\n","                des_running_loss.reset()\n","                des_acc.reset()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8yMShB4YHl2s","colab_type":"text"},"source":["# eval funcs"]},{"cell_type":"code","metadata":{"id":"tw6lPS_utNqS","colab_type":"code","colab":{}},"source":["from torch.nn import CosineSimilarity\n","\n","def evaluate_true_neg(epoch, data_iter, src_embed, model_dec,\n","             model_cls, cls_criteria, seq2seq_criteria,\n","             params):\n","    ''' Evaluate performances over test/validation dataloader '''\n","\n","    device = params.device\n","    trans_cls = params.TRANS_CLS\n","\n","    model_cls.eval()\n","    model_dec.eval()\n","\n","    cls_running_loss = Loss()\n","    rec_running_loss = Loss()\n","\n","    rec_acc = AccuracyRec()\n","    cls_acc = AccuracyCls()\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(data_iter):\n","            if params.TEST_MAX_BATCH_SIZE and i == params.TEST_MAX_BATCH_SIZE:\n","                break\n","\n","            # Prepare batch\n","            src, labels = batch.text, batch.label\n","            src_mask, _ = make_masks(src, src, device)\n","            src = src.to(device)\n","            src_mask = src_mask.to(device)\n","            labels = labels.to(device)\n","\n","            # Negate labels\n","            neg_labels = (~labels.byte()).long()\n","\n","            preds = model_dec(src, src_mask, neg_labels)\n","            neg_sent = torch.argmax(preds, dim=-1)\n","            preds_for_cls = src_embed(neg_sent)\n","            if trans_cls:\n","                cls_preds = model_cls(preds_for_cls, src_mask)\n","            else:\n","                cls_preds = model_cls(preds_for_cls)\n","\n","            # neg_sent_mask, _ = make_masks(neg_sent, neg_sent, device)\n","            preds = model_dec(neg_sent, src_mask, labels)\n","\n","            cls_loss = cls_criteria(cls_preds, neg_labels)\n","            cls_acc.update(cls_preds, neg_labels)\n","            cls_running_loss.update(cls_loss)\n","            preds = preds.contiguous().view(-1, preds.size(-1))\n","            src = src.contiguous().view(-1)\n","            rec_loss = seq2seq_criteria(preds, src)\n","            rec_running_loss.update(rec_loss)\n","            rec_acc.update(preds, src)\n","\n","    logging.info(\"Eval-e-{}: loss cls: {:.3f}, acc cls: {:.3f}, loss rec: {:.3f}, acc rec: {:.3f}\".format(epoch, cls_running_loss(),\n","                                                                                         cls_acc(), rec_running_loss(),\n","                                                                                         rec_acc()))\n","    \n","def greedy_decode_sent(preds, id2word, eos_id):\n","    ''' Nauve greedy decoding - just argmax over the vocabulary distribution '''\n","    preds = torch.argmax(preds, -1)\n","    decoded_sent = preds.squeeze(0).detach().cpu().numpy()\n","    # print(\" \".join([id2word[i] for i in decoded_sent]))\n","    decoded_sent = sent2str(decoded_sent, id2word, eos_id)\n","    return decoded_sent, preds\n","\n","\n","def sent2str(sent_as_np, id2word, eos_id=None):\n","    ''' Gets sentence as a list of ids and transfers to string\n","        Input is np array of ids '''\n","    if not (isinstance(sent_as_np, np.ndarray)):\n","        raise ValueError('Invalid input type, expected np array')\n","    if eos_id:\n","        end_id = np.where(sent_as_np == eos_id)[0]\n","        if len(end_id) > 1:\n","            sent_as_np = sent_as_np[:int(end_id[0])]\n","        elif len(end_id) == 1:\n","            sent_as_np = sent_as_np[:int(end_id)]\n","\n","    return \" \".join([id2word[i] for i in sent_as_np])\n","\n","\n","\n","def test_random_samples(data_iter, TEXT, model_dec, model_cls, device, src_embed=None, decode_func=None, num_samples=2,\n","                    transfer_style=True, trans_cls=False, embed_preds=False):\n","    ''' Print some sample text to validate the model.\n","        transfer_style - bool, if True apply style transfer '''\n","\n","    word2id = TEXT.vocab.stoi\n","    eos_id = int(word2id['<eos>'])\n","    id2word = {v: k for k, v in word2id.items()}\n","    model_dec.eval()\n","\n","    with torch.no_grad():\n","        for step, batch in enumerate(data_iter):\n","            if num_samples == 0: break\n","\n","            # Prepare batch\n","            src, labels = batch.text[0, ...], batch.label[0, ...]\n","            src = src.unsqueeze(0)\n","            labels = labels.unsqueeze(0)\n","            src_mask, _ = make_masks(src, src, device)\n","\n","            src = src.to(device)\n","            src_mask = src_mask.to(device)\n","            labels = labels.to(device)\n","            true_labels = copy.deepcopy(labels)\n","\n","            # Logical not on labels if transfer_style is set\n","            if transfer_style:\n","                labels = (~labels.byte()).long()\n","            # print(\"Original label \", true_labels, \" Transfer label \", labels)\n","            if src_embed:\n","                embeds = src_embed(src)\n","                preds = model_dec(embeds, src_mask, labels)\n","            else:\n","                preds = model_dec(src, src_mask, labels)\n","\n","            sent_as_list = src.squeeze(0).detach().cpu().numpy()\n","            src_sent = sent2str(sent_as_list, id2word, eos_id)\n","            src_label = 'pos' if true_labels.detach().item() == 1 else 'neg'\n","            logging.info('Original: text: {}'.format(src_sent))\n","            logging.info('Original: class: {}'.format(src_label))\n","\n","            if embed_preds:\n","                preds = preds_embedding_cosine_similarity(preds, model_dec.src_embed)\n","            if decode_func:\n","                dec_sent, decoded = decode_func(preds, id2word, eos_id)\n","                if src_embed:\n","                    decoded = src_embed(decoded)\n","                if trans_cls:\n","                    cls_preds = model_cls(decoded, src_mask)\n","                else:\n","                    cls_preds = model_cls(decoded)\n","                pred_label = 'pos' if torch.argmax(cls_preds) == 1 else 'neg'\n","                if transfer_style:\n","                    logging.info('Style transfer output:')\n","                logging.info('Predicted: text: {}'.format(dec_sent))\n","                logging.info('Predicted: class: {}'.format(pred_label))\n","\n","            else:\n","                logging.info('Predicted: class: {}'.format(pred_label))\n","            logging.info('\\n')\n","\n","            num_samples -= 1\n","\n","def test_user_string(sent, label, TEXT, model_dec, model_cls, device, decode_func=None,\n","                    transfer_style=True, trans_cls=False, embed_preds=False):\n","    ''' Print some sample text to validate the model.\n","        transfer_style - bool, if True apply style transfer '''\n","\n","    word2id = TEXT.vocab.stoi\n","    eos_id = int(word2id['<eos>'])\n","    id2word = {v: k for k, v in word2id.items()}\n","    # define tokenizer\n","    en = English()\n","    def id_tokenize(sentence):\n","        return [word2id[tok.text] for tok in en.tokenizer(sentence)]\n","    \n","    model_dec.eval()\n","\n","    with torch.no_grad():\n","        # Prepare batch\n","        \n","        token_ids = id_tokenize[sent]\n","        src = torch.LongTensor(token_ids)\n","        labels = torch.LongTensor(label).unsqueeze(0)\n","        src_mask, _ = make_masks(src, src, device)\n","\n","        src = src.to(device)\n","        src_mask = src_mask.to(device)\n","        labels = labels.to(device)\n","        true_labels = copy.deepcopy(labels)\n","\n","        # Logical not on labels if transfer_style is set\n","        if transfer_style:\n","            labels = (~labels.byte()).long()\n","        print(labels, true_labels)\n","\n","        preds = model_dec(src, src_mask, labels)\n","\n","        src_label = 'pos' if true_labels.detach().item() == 1 else 'neg'\n","        logging.info('Original: text: {}'.format(src_sent))\n","        logging.info('Original: class: {}'.format(src_label))\n","\n","        if embed_preds:\n","            preds = preds_embedding_cosine_similarity(preds, model_dec.src_embed)\n","        if decode_func:\n","            dec_sent, decoded = decode_func(preds, id2word, eos_id)\n","            preds_for_cls = model_dec.src_embed(decoded)\n","            if trans_cls:\n","                cls_preds = model_cls(preds_for_cls, src_mask)\n","            else:\n","                cls_preds = model_cls(preds_for_cls)\n","            pred_label = 'pos' if torch.argmax(cls_preds) == 1 else 'neg'\n","            if transfer_style:\n","                logging.info('Style transfer output:')\n","            logging.info('Predicted: text: {}'.format(dec_sent))\n","            logging.info('Predicted: class: {}'.format(pred_label))\n","\n","        else:\n","            logging.info('Predicted: class: {}'.format(pred_label))\n","        logging.info('\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rV1qekxIMl3t","colab_type":"text"},"source":["# pretrain generator"]},{"cell_type":"code","metadata":{"id":"kv5a2i7tMoQU","colab_type":"code","outputId":"bee2d256-34c9-4975-babc-cf44f32cb228","executionInfo":{"status":"ok","timestamp":1568216331273,"user_tz":-180,"elapsed":50946,"user":{"displayName":"Gal Shachaf","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAmvlpUu2oQV5trCyjFvlOX1Yp8tPVeVSHp3KeehEc=s64","userId":"09922006514276555207"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["for epoch in range(1):\n","    verbose = params.VERBOSE\n","    device = params.device\n","\n","    rec_running_loss = Loss()\n","    rec_acc = AccuracyRec()\n","\n","    model_dec.train()\n","    # preds = torch.FloatTensor(params.TRAIN_BATCH_SIZE, params.MAX_LEN, vocab_size)\n","    # preds = preds.to(device)\n","    for step, batch in enumerate(train_iter):\n","        # prepare batch\n","        src, labels = batch.text, batch.label\n","        src_mask, _ = make_masks(src, src, device)\n","\n","        src = src.to(device)\n","        src_mask = src_mask.to(device)\n","        labels = labels.to(device)\n","        # neg_labels = (~labels.byte()).long()\n","\n","        # neg_preds = model_dec(src, src_mask, neg_labels, argmax=False)\n","        preds = model_dec(src, src_mask, labels, argmax=False)\n","        # preds = model_dec(neg_preds, src_mask, labels, argmax=True)\n","        preds = preds.contiguous().view(-1, preds.size(-1))\n","        src = src.contiguous().view(-1)\n","        rec_loss = seq2seq_criteria(preds, src)\n","        rec_running_loss.update(rec_loss)\n","        rec_acc.update(preds, src)\n","\n","        # optimize decoder\n","        loss = rec_loss\n","        opt_dec.zero_grad()\n","        loss.backward()\n","        opt_dec.step()\n","\n","        if verbose and step % 20 == 19:\n","            logging.info(\n","            \"e-{},s-{}: Pre-Training transformer on rec, rec_loss {}, rec_acc {}\".format(epoch,\n","                                                                                        step,\n","                                                                                        rec_running_loss(),\n","                                                                                        rec_acc()))\n","            if rec_acc() >= 50.0:\n","                break\n","            rec_running_loss.reset()\n","            rec_acc.reset()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO - 09/11/19 15:38:04 - 1:19:03 - e-0,s-19: Pre-Training transformer on rec, rec_loss 9.647946643829346, rec_acc 0.0710025560920193\n","INFO - 09/11/19 15:38:07 - 1:19:06 - e-0,s-39: Pre-Training transformer on rec, rec_loss 9.534781312942505, rec_acc 7.9977706562630635\n","INFO - 09/11/19 15:38:10 - 1:19:08 - e-0,s-59: Pre-Training transformer on rec, rec_loss 9.344154119491577, rec_acc 20.058139534883722\n","INFO - 09/11/19 15:38:12 - 1:19:11 - e-0,s-79: Pre-Training transformer on rec, rec_loss 9.125034761428832, rec_acc 20.858895705521473\n","INFO - 09/11/19 15:38:15 - 1:19:14 - e-0,s-99: Pre-Training transformer on rec, rec_loss 8.913456058502197, rec_acc 19.505219505219507\n","INFO - 09/11/19 15:38:18 - 1:19:17 - e-0,s-119: Pre-Training transformer on rec, rec_loss 8.661820554733277, rec_acc 19.826743048763447\n","INFO - 09/11/19 15:38:21 - 1:19:19 - e-0,s-139: Pre-Training transformer on rec, rec_loss 8.391303777694702, rec_acc 23.21124361158433\n","INFO - 09/11/19 15:38:23 - 1:19:22 - e-0,s-159: Pre-Training transformer on rec, rec_loss 8.10493357181549, rec_acc 25.469433855710854\n","INFO - 09/11/19 15:38:26 - 1:19:25 - e-0,s-179: Pre-Training transformer on rec, rec_loss 7.785476803779602, rec_acc 30.031091011871112\n","INFO - 09/11/19 15:38:29 - 1:19:28 - e-0,s-199: Pre-Training transformer on rec, rec_loss 7.452654600143433, rec_acc 34.678645473393225\n","INFO - 09/11/19 15:38:32 - 1:19:30 - e-0,s-219: Pre-Training transformer on rec, rec_loss 7.1016803741455075, rec_acc 36.92609182530795\n","INFO - 09/11/19 15:38:34 - 1:19:33 - e-0,s-239: Pre-Training transformer on rec, rec_loss 6.746204614639282, rec_acc 37.805571347356455\n","INFO - 09/11/19 15:38:37 - 1:19:36 - e-0,s-259: Pre-Training transformer on rec, rec_loss 6.359862375259399, rec_acc 41.03453258199682\n","INFO - 09/11/19 15:38:40 - 1:19:39 - e-0,s-279: Pre-Training transformer on rec, rec_loss 5.9799833536148075, rec_acc 43.62151917189095\n","INFO - 09/11/19 15:38:43 - 1:19:41 - e-0,s-299: Pre-Training transformer on rec, rec_loss 5.624848246574402, rec_acc 44.779582366589324\n","INFO - 09/11/19 15:38:45 - 1:19:44 - e-0,s-319: Pre-Training transformer on rec, rec_loss 5.362447214126587, rec_acc 45.627050934512766\n","INFO - 09/11/19 15:38:48 - 1:19:47 - e-0,s-339: Pre-Training transformer on rec, rec_loss 4.899553012847901, rec_acc 49.23388262503614\n","INFO - 09/11/19 15:38:51 - 1:19:50 - e-0,s-359: Pre-Training transformer on rec, rec_loss 4.593761420249939, rec_acc 50.58382586132334\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"v3fFaLAzmPGF","colab_type":"text"},"source":["# pretrain cls"]},{"cell_type":"code","metadata":{"id":"9flVhSHMmOFK","colab_type":"code","outputId":"de8cfa4e-ba78-4fa2-eabb-a66b3147b2a8","executionInfo":{"status":"ok","timestamp":1568213615481,"user_tz":-180,"elapsed":364659,"user":{"displayName":"Gal Shachaf","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAmvlpUu2oQV5trCyjFvlOX1Yp8tPVeVSHp3KeehEc=s64","userId":"09922006514276555207"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from torch.nn import functional as F\n","# model_cls = model_des\n","# opt_cls = opt_des\n","\n","# def fill_one_hot_src(src, y_onehot):\n","#     y_onehot.zero_()\n","#     y_onehot.scatter_(dim=-1, index=src.unsqueeze(-1), value=1)\n","\n","for epoch in range(1):\n","    verbose = params.VERBOSE\n","    device = params.device\n","\n","    cls_running_loss = Loss()\n","    cls_acc = AccuracyCls()\n","\n","    model_cls.train()\n","    # preds = torch.FloatTensor(params.TRAIN_BATCH_SIZE, params.MAX_LEN, vocab_size)\n","    # preds = preds.to(device)\n","    for step, batch in enumerate(train_iter):\n","        # prepare batch\n","        src, labels = batch.text, batch.label\n","        # if src.shape[0] != preds.shape[0]:\n","        #     continue\n","        src_mask, _ = make_masks(src, src, device)\n","\n","        src = src.to(device)\n","        src_mask = src_mask.to(device)\n","        labels = labels.to(device)\n","        if params.TRANS_CLS:\n","            cls_preds = model_cls(src, src_mask, argmax=False)\n","        else:\n","            cls_preds = model_cls(src)\n","        \n","        opt_cls.zero_grad()\n","        cls_loss = cls_criteria(cls_preds, labels)\n","        cls_acc.update(cls_preds, labels)\n","        cls_running_loss.update(cls_loss)\n","        cls_loss.backward()\n","        opt_cls.step()\n","\n","        # train_true_neg_cls_step(model_dec, model_cls, cls_criteria,\n","        #                 opt_dec, opt_cls,\n","        #                 src, src_mask, labels,  cls_running_loss,\n","        #                 cls_acc, device=device, trans_cls=params.TRANS_CLS)\n","\n","\n","        if verbose and step%100 == 99:\n","            logging.info(\n","                \"e-{},s-{}: Training cls loss {} acc {}\".format(epoch, step, cls_running_loss(),\n","                                                                cls_acc()))\n","            cls_running_loss.reset()\n","            cls_acc.reset()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO - 09/11/19 14:47:37 - 0:28:35 - e-0,s-99: Training cls loss 0.72555723965168 acc 55.25\n","INFO - 09/11/19 14:47:42 - 0:28:40 - e-0,s-199: Training cls loss 0.5212832516431809 acc 77.3125\n","INFO - 09/11/19 14:47:46 - 0:28:45 - e-0,s-299: Training cls loss 0.3840521217882633 acc 89.0625\n","INFO - 09/11/19 14:47:51 - 0:28:50 - e-0,s-399: Training cls loss 0.32129169657826423 acc 93.3125\n","INFO - 09/11/19 14:47:56 - 0:28:55 - e-0,s-499: Training cls loss 0.31894743502140044 acc 92.90625\n","INFO - 09/11/19 14:48:01 - 0:29:00 - e-0,s-599: Training cls loss 0.30550199940800665 acc 93.5625\n","INFO - 09/11/19 14:48:06 - 0:29:05 - e-0,s-699: Training cls loss 0.28642746567726135 acc 95.40625\n","INFO - 09/11/19 14:48:11 - 0:29:10 - e-0,s-799: Training cls loss 0.29774092480540276 acc 94.3125\n","INFO - 09/11/19 14:48:16 - 0:29:15 - e-0,s-899: Training cls loss 0.28578834801912306 acc 95.03125\n","INFO - 09/11/19 14:48:21 - 0:29:20 - e-0,s-999: Training cls loss 0.2721908695995808 acc 96.09375\n","INFO - 09/11/19 14:48:26 - 0:29:25 - e-0,s-1099: Training cls loss 0.2740606960654259 acc 95.90625\n","INFO - 09/11/19 14:48:31 - 0:29:30 - e-0,s-1199: Training cls loss 0.2655697800219059 acc 96.40625\n","INFO - 09/11/19 14:48:36 - 0:29:35 - e-0,s-1299: Training cls loss 0.25615947470068934 acc 97.15625\n","INFO - 09/11/19 14:48:41 - 0:29:40 - e-0,s-1399: Training cls loss 0.25737307041883467 acc 97.15625\n","INFO - 09/11/19 14:48:46 - 0:29:44 - e-0,s-1499: Training cls loss 0.26560683876276014 acc 96.40625\n","INFO - 09/11/19 14:48:51 - 0:29:49 - e-0,s-1599: Training cls loss 0.2621812070906162 acc 96.71875\n","INFO - 09/11/19 14:48:56 - 0:29:54 - e-0,s-1699: Training cls loss 0.2626689201593399 acc 96.5625\n","INFO - 09/11/19 14:49:00 - 0:29:59 - e-0,s-1799: Training cls loss 0.2541854465007782 acc 97.21875\n","INFO - 09/11/19 14:49:05 - 0:30:04 - e-0,s-1899: Training cls loss 0.24701420426368714 acc 97.8125\n","INFO - 09/11/19 14:49:10 - 0:30:09 - e-0,s-1999: Training cls loss 0.2513924220204353 acc 97.46875\n","INFO - 09/11/19 14:49:15 - 0:30:14 - e-0,s-2099: Training cls loss 0.25124404653906823 acc 97.4375\n","INFO - 09/11/19 14:49:20 - 0:30:19 - e-0,s-2199: Training cls loss 0.24801312863826752 acc 97.6875\n","INFO - 09/11/19 14:49:25 - 0:30:24 - e-0,s-2299: Training cls loss 0.2470909821987152 acc 98.0\n","INFO - 09/11/19 14:49:30 - 0:30:29 - e-0,s-2399: Training cls loss 0.25259306758642197 acc 97.5\n","INFO - 09/11/19 14:49:35 - 0:30:34 - e-0,s-2499: Training cls loss 0.24288398683071136 acc 98.125\n","INFO - 09/11/19 14:49:40 - 0:30:39 - e-0,s-2599: Training cls loss 0.24125706985592843 acc 98.21875\n","INFO - 09/11/19 14:49:45 - 0:30:44 - e-0,s-2699: Training cls loss 0.2407502429187298 acc 98.34375\n","INFO - 09/11/19 14:49:50 - 0:30:49 - e-0,s-2799: Training cls loss 0.25530137911438944 acc 97.28125\n","INFO - 09/11/19 14:49:55 - 0:30:54 - e-0,s-2899: Training cls loss 0.24721319064497949 acc 97.75\n","INFO - 09/11/19 14:50:00 - 0:30:58 - e-0,s-2999: Training cls loss 0.2417389141023159 acc 98.3125\n","INFO - 09/11/19 14:50:05 - 0:31:03 - e-0,s-3099: Training cls loss 0.24680451050400734 acc 97.84375\n","INFO - 09/11/19 14:50:09 - 0:31:08 - e-0,s-3199: Training cls loss 0.2474378776550293 acc 97.71875\n","INFO - 09/11/19 14:50:14 - 0:31:13 - e-0,s-3299: Training cls loss 0.240978884100914 acc 98.375\n","INFO - 09/11/19 14:50:19 - 0:31:18 - e-0,s-3399: Training cls loss 0.24757253453135492 acc 97.8125\n","INFO - 09/11/19 14:50:24 - 0:31:23 - e-0,s-3499: Training cls loss 0.2472316049039364 acc 97.8125\n","INFO - 09/11/19 14:50:29 - 0:31:28 - e-0,s-3599: Training cls loss 0.24636421099305153 acc 97.875\n","INFO - 09/11/19 14:50:34 - 0:31:33 - e-0,s-3699: Training cls loss 0.24968186885118485 acc 97.6875\n","INFO - 09/11/19 14:50:39 - 0:31:38 - e-0,s-3799: Training cls loss 0.243720436245203 acc 98.03125\n","INFO - 09/11/19 14:50:44 - 0:31:43 - e-0,s-3899: Training cls loss 0.24787481755018234 acc 97.71875\n","INFO - 09/11/19 14:50:49 - 0:31:48 - e-0,s-3999: Training cls loss 0.24437113463878632 acc 98.0\n","INFO - 09/11/19 14:50:54 - 0:31:53 - e-0,s-4099: Training cls loss 0.24834177330136298 acc 97.625\n","INFO - 09/11/19 14:50:59 - 0:31:58 - e-0,s-4199: Training cls loss 0.2406095689535141 acc 98.3125\n","INFO - 09/11/19 14:51:04 - 0:32:03 - e-0,s-4299: Training cls loss 0.24496368288993836 acc 98.0\n","INFO - 09/11/19 14:51:09 - 0:32:07 - e-0,s-4399: Training cls loss 0.24036359667778015 acc 98.375\n","INFO - 09/11/19 14:51:14 - 0:32:12 - e-0,s-4499: Training cls loss 0.24195297956466674 acc 98.21875\n","INFO - 09/11/19 14:51:19 - 0:32:17 - e-0,s-4599: Training cls loss 0.23645830884575844 acc 98.65625\n","INFO - 09/11/19 14:51:23 - 0:32:22 - e-0,s-4699: Training cls loss 0.23619567602872849 acc 98.6875\n","INFO - 09/11/19 14:51:28 - 0:32:27 - e-0,s-4799: Training cls loss 0.23665411934256553 acc 98.65625\n","INFO - 09/11/19 14:51:33 - 0:32:32 - e-0,s-4899: Training cls loss 0.23221358820796012 acc 98.9375\n","INFO - 09/11/19 14:51:38 - 0:32:37 - e-0,s-4999: Training cls loss 0.24379729956388474 acc 98.09375\n","INFO - 09/11/19 14:51:43 - 0:32:42 - e-0,s-5099: Training cls loss 0.23988156363368035 acc 98.375\n","INFO - 09/11/19 14:51:48 - 0:32:47 - e-0,s-5199: Training cls loss 0.2398696567118168 acc 98.375\n","INFO - 09/11/19 14:51:53 - 0:32:52 - e-0,s-5299: Training cls loss 0.23644933685660363 acc 98.625\n","INFO - 09/11/19 14:51:58 - 0:32:57 - e-0,s-5399: Training cls loss 0.23620108038187027 acc 98.59375\n","INFO - 09/11/19 14:52:03 - 0:33:02 - e-0,s-5499: Training cls loss 0.2374469929933548 acc 98.5\n","INFO - 09/11/19 14:52:08 - 0:33:07 - e-0,s-5599: Training cls loss 0.23602019712328912 acc 98.53125\n","INFO - 09/11/19 14:52:13 - 0:33:12 - e-0,s-5699: Training cls loss 0.23570002019405364 acc 98.625\n","INFO - 09/11/19 14:52:18 - 0:33:17 - e-0,s-5799: Training cls loss 0.23175698176026344 acc 99.03125\n","INFO - 09/11/19 14:52:23 - 0:33:22 - e-0,s-5899: Training cls loss 0.23446618974208833 acc 98.8125\n","INFO - 09/11/19 14:52:28 - 0:33:26 - e-0,s-5999: Training cls loss 0.2355593465268612 acc 98.71875\n","INFO - 09/11/19 14:52:33 - 0:33:31 - e-0,s-6099: Training cls loss 0.23395198345184326 acc 98.8125\n","INFO - 09/11/19 14:52:37 - 0:33:36 - e-0,s-6199: Training cls loss 0.23513296887278556 acc 98.75\n","INFO - 09/11/19 14:52:42 - 0:33:41 - e-0,s-6299: Training cls loss 0.22940754801034927 acc 99.1875\n","INFO - 09/11/19 14:52:47 - 0:33:46 - e-0,s-6399: Training cls loss 0.23258885011076927 acc 98.875\n","INFO - 09/11/19 14:52:52 - 0:33:51 - e-0,s-6499: Training cls loss 0.23043712481856346 acc 99.03125\n","INFO - 09/11/19 14:52:57 - 0:33:56 - e-0,s-6599: Training cls loss 0.23052022069692613 acc 99.03125\n","INFO - 09/11/19 14:53:02 - 0:34:01 - e-0,s-6699: Training cls loss 0.2338351984322071 acc 98.78125\n","INFO - 09/11/19 14:53:07 - 0:34:06 - e-0,s-6799: Training cls loss 0.2316606292128563 acc 98.96875\n","INFO - 09/11/19 14:53:12 - 0:34:11 - e-0,s-6899: Training cls loss 0.2341172255575657 acc 98.84375\n","INFO - 09/11/19 14:53:17 - 0:34:16 - e-0,s-6999: Training cls loss 0.2329430389404297 acc 98.875\n","INFO - 09/11/19 14:53:22 - 0:34:21 - e-0,s-7099: Training cls loss 0.2336367216706276 acc 98.78125\n","INFO - 09/11/19 14:53:27 - 0:34:26 - e-0,s-7199: Training cls loss 0.2321302765607834 acc 98.90625\n","INFO - 09/11/19 14:53:32 - 0:34:31 - e-0,s-7299: Training cls loss 0.23002522960305213 acc 99.09375\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_kpdaERHUx2d","colab_type":"code","colab":{}},"source":["model_cls_path = os.path.join(params.MODELS_PATH, \"model_cls_{}_yelp_freq{}_len{}_dim_{}.pth\".format(params.N_LAYERS_CLS, params.VOCAB_MIN_FREQ, params.MAX_LEN, params.H_DIM))\n","model_dec_path = os.path.join(params.MODELS_PATH, \"model_dec_{}_yelp_freq{}_len{}_dim{}_pretrain_50.pth\".format(params.N_LAYERS, params.VOCAB_MIN_FREQ, params.MAX_LEN, params.H_DIM))\n","\n","torch.save(model_cls.state_dict(), model_cls_path)\n","torch.save(model_dec.state_dict(), model_dec_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yrA0QXnyax-p","colab_type":"code","colab":{}},"source":["model_cls_path = os.path.join(params.MODELS_PATH, \"model_cls_{}_yelp_freq{}_len{}_dim{}.pth\".format(params.N_LAYERS_CLS, params.VOCAB_MIN_FREQ, params.MAX_LEN, params.H_DIM))\n","model_dec_path = os.path.join(params.MODELS_PATH, \"model_dec_{}_yelp_freq{}_len{}.pth_dim{}\".format(params.N_LAYERS, params.VOCAB_MIN_FREQ, params.MAX_LEN, params.H_DIM))\n","\n","model_cls.load_state_dict(torch.load(model_cls_path))\n","model_dec.load_state_dict(torch.load(model_dec_path))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mHLmp5YUHqyN","colab_type":"text"},"source":["# run"]},{"cell_type":"code","metadata":{"id":"LPcxyVR2PSIk","colab_type":"code","outputId":"6f6045ba-4b22-429e-e4b6-420c9829caf7","executionInfo":{"status":"error","timestamp":1568217658456,"user_tz":-180,"elapsed":1326575,"user":{"displayName":"Gal Shachaf","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAmvlpUu2oQV5trCyjFvlOX1Yp8tPVeVSHp3KeehEc=s64","userId":"09922006514276555207"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for epoch in range(params.N_EPOCHS):\n","    model_dec_path = os.path.join(params.MODELS_PATH, \"model_dec_{}_yelp_freq{}_len{}_dim{}_sample_e{}.pth\".format(params.N_LAYERS, params.VOCAB_MIN_FREQ, params.MAX_LEN, params.H_DIM, epoch))\n","    run_epoch_true_neg(epoch=epoch, data_iter=train_iter,\n","                model_dec=model_dec, opt_dec=opt_dec, model_des=None, opt_des=None,\n","                model_cls=model_cls, opt_cls=opt_cls, cls_criteria=cls_criteria,\n","                seq2seq_criteria=seq2seq_criteria, des_criteria=des_criteria,\n","                params=params)\n","    torch.save(model_dec.state_dict(), model_dec_path)\n","#   test_acc = evaluate_true_neg(epoch, test_iter, model_dec,\n","#                       model_cls, cls_criteria, seq2seq_criteria,\n","#                       params)\n","\n","    test_random_samples(train_iter, TEXT, model_dec, model_cls, params.device,\n","                        decode_func=greedy_decode_sent, num_samples=5, transfer_style=True,\n","                        trans_cls=params.TRANS_CLS)\n","\n","  # TODO - Roy - currently not in use, what metric to follow ?\n","  # early_stop(test_acc)\n","  # if early_stop.early_stop:\n","  #     break\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO - 09/11/19 15:38:53 - 1:19:52 - total epoch steps 7366, period size 100\n","INFO - 09/11/19 15:39:20 - 1:20:19 - e-0,s-99: Trained transformer on negated label, cls_loss 0.784, cls_acc 55.250, rec_loss 4.524, rec_acc 54.759, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:39:47 - 1:20:45 - e-0,s-199: Trained transformer on negated label, cls_loss 0.807, cls_acc 54.875, rec_loss 3.854, rec_acc 58.046, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:40:13 - 1:21:12 - e-0,s-299: Trained transformer on negated label, cls_loss 0.810, cls_acc 55.094, rec_loss 3.416, rec_acc 60.743, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:40:39 - 1:21:38 - e-0,s-399: Trained transformer on negated label, cls_loss 0.806, cls_acc 55.156, rec_loss 3.020, rec_acc 65.240, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:41:06 - 1:22:04 - e-0,s-499: Trained transformer on negated label, cls_loss 0.791, cls_acc 56.563, rec_loss 2.732, rec_acc 68.196, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:41:32 - 1:22:31 - e-0,s-599: Trained transformer on negated label, cls_loss 0.810, cls_acc 55.031, rec_loss 2.569, rec_acc 70.511, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:41:58 - 1:22:57 - e-0,s-699: Trained transformer on negated label, cls_loss 0.832, cls_acc 53.656, rec_loss 2.298, rec_acc 73.490, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:42:25 - 1:23:24 - e-0,s-799: Trained transformer on negated label, cls_loss 0.821, cls_acc 54.125, rec_loss 2.117, rec_acc 75.888, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:42:51 - 1:23:50 - e-0,s-899: Trained transformer on negated label, cls_loss 0.818, cls_acc 54.219, rec_loss 1.942, rec_acc 77.899, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:43:18 - 1:24:16 - e-0,s-999: Trained transformer on negated label, cls_loss 0.810, cls_acc 55.031, rec_loss 1.787, rec_acc 80.041, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:43:44 - 1:24:43 - e-0,s-1099: Trained transformer on negated label, cls_loss 0.795, cls_acc 56.469, rec_loss 1.659, rec_acc 81.615, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:44:10 - 1:25:09 - e-0,s-1199: Trained transformer on negated label, cls_loss 0.805, cls_acc 55.188, rec_loss 1.610, rec_acc 82.522, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:44:37 - 1:25:36 - e-0,s-1299: Trained transformer on negated label, cls_loss 0.819, cls_acc 54.250, rec_loss 1.479, rec_acc 84.124, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:45:03 - 1:26:02 - e-0,s-1399: Trained transformer on negated label, cls_loss 0.801, cls_acc 55.750, rec_loss 1.400, rec_acc 84.709, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:45:30 - 1:26:28 - e-0,s-1499: Trained transformer on negated label, cls_loss 0.804, cls_acc 55.594, rec_loss 1.335, rec_acc 85.548, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:45:56 - 1:26:55 - e-0,s-1599: Trained transformer on negated label, cls_loss 0.834, cls_acc 52.969, rec_loss 1.249, rec_acc 86.350, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:46:23 - 1:27:21 - e-0,s-1699: Trained transformer on negated label, cls_loss 0.807, cls_acc 55.281, rec_loss 1.202, rec_acc 87.066, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:46:49 - 1:27:48 - e-0,s-1799: Trained transformer on negated label, cls_loss 0.817, cls_acc 54.438, rec_loss 1.158, rec_acc 87.612, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:47:15 - 1:28:14 - e-0,s-1899: Trained transformer on negated label, cls_loss 0.796, cls_acc 56.094, rec_loss 1.114, rec_acc 88.272, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:47:42 - 1:28:41 - e-0,s-1999: Trained transformer on negated label, cls_loss 0.827, cls_acc 53.438, rec_loss 1.084, rec_acc 88.500, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:48:08 - 1:29:07 - e-0,s-2099: Trained transformer on negated label, cls_loss 0.808, cls_acc 54.812, rec_loss 1.055, rec_acc 88.879, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:48:35 - 1:29:34 - e-0,s-2199: Trained transformer on negated label, cls_loss 0.795, cls_acc 55.937, rec_loss 1.020, rec_acc 89.064, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:49:01 - 1:30:00 - e-0,s-2299: Trained transformer on negated label, cls_loss 0.796, cls_acc 55.844, rec_loss 1.032, rec_acc 89.043, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:49:28 - 1:30:26 - e-0,s-2399: Trained transformer on negated label, cls_loss 0.815, cls_acc 54.312, rec_loss 0.959, rec_acc 89.633, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:49:54 - 1:30:53 - e-0,s-2499: Trained transformer on negated label, cls_loss 0.803, cls_acc 55.437, rec_loss 0.962, rec_acc 89.307, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:50:20 - 1:31:19 - e-0,s-2599: Trained transformer on negated label, cls_loss 0.797, cls_acc 55.469, rec_loss 0.986, rec_acc 88.605, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:50:47 - 1:31:46 - e-0,s-2699: Trained transformer on negated label, cls_loss 0.818, cls_acc 54.125, rec_loss 1.011, rec_acc 88.296, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:51:13 - 1:32:12 - e-0,s-2799: Trained transformer on negated label, cls_loss 0.799, cls_acc 55.281, rec_loss 0.958, rec_acc 88.704, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:51:40 - 1:32:38 - e-0,s-2899: Trained transformer on negated label, cls_loss 0.808, cls_acc 54.812, rec_loss 0.936, rec_acc 88.937, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:52:06 - 1:33:05 - e-0,s-2999: Trained transformer on negated label, cls_loss 0.817, cls_acc 54.031, rec_loss 0.925, rec_acc 88.934, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:52:32 - 1:33:31 - e-0,s-3099: Trained transformer on negated label, cls_loss 0.826, cls_acc 53.250, rec_loss 0.914, rec_acc 89.141, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:52:59 - 1:33:58 - e-0,s-3199: Trained transformer on negated label, cls_loss 0.813, cls_acc 54.281, rec_loss 0.881, rec_acc 89.424, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:53:25 - 1:34:24 - e-0,s-3299: Trained transformer on negated label, cls_loss 0.804, cls_acc 54.906, rec_loss 0.864, rec_acc 89.542, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:53:52 - 1:34:50 - e-0,s-3399: Trained transformer on negated label, cls_loss 0.814, cls_acc 54.188, rec_loss 0.848, rec_acc 89.627, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:54:18 - 1:35:17 - e-0,s-3499: Trained transformer on negated label, cls_loss 0.824, cls_acc 53.344, rec_loss 0.854, rec_acc 89.841, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:54:44 - 1:35:43 - e-0,s-3599: Trained transformer on negated label, cls_loss 0.796, cls_acc 55.250, rec_loss 0.847, rec_acc 89.725, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:55:11 - 1:36:10 - e-0,s-3699: Trained transformer on negated label, cls_loss 0.793, cls_acc 56.094, rec_loss 0.829, rec_acc 90.020, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:55:37 - 1:36:36 - e-0,s-3799: Trained transformer on negated label, cls_loss 0.809, cls_acc 54.750, rec_loss 0.825, rec_acc 90.039, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:56:04 - 1:37:03 - e-0,s-3899: Trained transformer on negated label, cls_loss 0.798, cls_acc 55.437, rec_loss 0.792, rec_acc 90.324, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:56:30 - 1:37:29 - e-0,s-3999: Trained transformer on negated label, cls_loss 0.808, cls_acc 54.406, rec_loss 0.819, rec_acc 90.027, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:56:57 - 1:37:55 - e-0,s-4099: Trained transformer on negated label, cls_loss 0.815, cls_acc 54.094, rec_loss 0.817, rec_acc 89.948, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:57:23 - 1:38:22 - e-0,s-4199: Trained transformer on negated label, cls_loss 0.803, cls_acc 54.844, rec_loss 0.827, rec_acc 89.980, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:57:49 - 1:38:48 - e-0,s-4299: Trained transformer on negated label, cls_loss 0.804, cls_acc 54.844, rec_loss 0.808, rec_acc 90.017, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:58:16 - 1:39:15 - e-0,s-4399: Trained transformer on negated label, cls_loss 0.797, cls_acc 55.562, rec_loss 0.807, rec_acc 90.150, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:58:42 - 1:39:41 - e-0,s-4499: Trained transformer on negated label, cls_loss 0.798, cls_acc 55.375, rec_loss 0.791, rec_acc 90.432, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:59:09 - 1:40:07 - e-0,s-4599: Trained transformer on negated label, cls_loss 0.826, cls_acc 53.125, rec_loss 0.830, rec_acc 89.920, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 15:59:35 - 1:40:34 - e-0,s-4699: Trained transformer on negated label, cls_loss 0.809, cls_acc 54.281, rec_loss 0.776, rec_acc 90.279, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 16:00:01 - 1:41:00 - e-0,s-4799: Trained transformer on negated label, cls_loss 0.799, cls_acc 55.312, rec_loss 0.776, rec_acc 90.238, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 16:00:28 - 1:41:27 - e-0,s-4899: Trained transformer on negated label, cls_loss 0.808, cls_acc 54.875, rec_loss 0.788, rec_acc 90.127, des_loss 0.000, des_acc 0.000\n","INFO - 09/11/19 16:00:54 - 1:41:53 - e-0,s-4999: Trained transformer on negated label, cls_loss 0.817, cls_acc 53.781, rec_loss 0.824, rec_acc 89.779, des_loss 0.000, des_acc 0.000\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-178-a27b139de41b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mmodel_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_criteria\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls_criteria\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mseq2seq_criteria\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq2seq_criteria\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdes_criteria\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdes_criteria\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 params=params)\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dec_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#   test_acc = evaluate_true_neg(epoch, test_iter, model_dec,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-155-c42add955c9e>\u001b[0m in \u001b[0;36mrun_epoch_true_neg\u001b[0;34m(epoch, data_iter, model_dec, opt_dec, model_cls, opt_cls, model_des, opt_des, cls_criteria, des_criteria, seq2seq_criteria, params)\u001b[0m\n\u001b[1;32m    175\u001b[0m                                  \u001b[0mopt_dec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_running_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrec_running_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrec_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_running_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls_running_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                                  \u001b[0mcls_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRANS_CLS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcyc_rec_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEG_CYC_REC_LAMBDA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                                  rec_lambda=params.NEG_REC_LAMBDA, cls_lambda=params.NEG_CLS_LAMBDA, des_lambda=params.NEG_DES_LAMBDA)\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mcurr_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-155-c42add955c9e>\u001b[0m in \u001b[0;36mtrain_neg_label_step\u001b[0;34m(model_dec, seq2seq_criteria, model_cls, model_des, cls_criteria, des_criteria, opt_dec, src, src_mask, labels, rec_running_loss, rec_acc, cls_running_loss, cls_acc, des_running_loss, des_acc, device, trans_cls, cyc_rec_lambda, cls_lambda, rec_lambda, des_lambda)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Negate labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mneg_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mneg_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# true_preds = model_dec(src, src_mask, labels, argmax=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrans_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-166-f8d5e73e7b71>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, style, argmax)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# add style before position?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0menc_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/NLP/final/TextualStyleTransfer/transformer_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;34m\"Pass the input (and mask) through each layer in turn.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/NLP/final/TextualStyleTransfer/transformer_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;34m\"Follow Figure 1 (left) for connections.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/NLP/final/TextualStyleTransfer/transformer_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;34m\"Apply residual connection to any sublayer with the same size.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/NLP/final/TextualStyleTransfer/transformer_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"01GLLGE3DoBj","colab_type":"code","outputId":"0ebff115-4698-46cc-816d-a506cd485e25","executionInfo":{"status":"ok","timestamp":1568222000316,"user_tz":-180,"elapsed":2115,"user":{"displayName":"Gal Shachaf","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAmvlpUu2oQV5trCyjFvlOX1Yp8tPVeVSHp3KeehEc=s64","userId":"09922006514276555207"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["test_iter.shuffle = False\n","\n","test_random_samples(train_iter, TEXT, model_dec, model_cls, params.device,\n","                    decode_func=greedy_decode_sent, num_samples=20, transfer_style=True,\n","                    trans_cls=params.TRANS_CLS)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO - 09/11/19 17:13:19 - 2:54:18 - Original: text: just simple hands on learning from very knowledgable staff!.\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: class: neg\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Style transfer output:\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: text: just simple hands on experience from very patrons 1997\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: class: neg\n","INFO - 09/11/19 17:13:19 - 2:54:18 - \n","                                     \n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: text: the esthetician is amazing as well .\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: class: neg\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Style transfer output:\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: text: the awesome is disgusting as well .\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: class: pos\n","INFO - 09/11/19 17:13:19 - 2:54:18 - \n","                                     \n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: text: we will definitely be back!.\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: class: neg\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Style transfer output:\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: text: we will\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: class: neg\n","INFO - 09/11/19 17:13:19 - 2:54:18 - \n","                                     \n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: text: the chinese menu items are also very tasty .\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: class: neg\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Style transfer output:\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: text: the asian menu items are also very flavorless .\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: class: pos\n","INFO - 09/11/19 17:13:19 - 2:54:18 - \n","                                     \n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: text: everything was delicious ! dave was our waiter and he was fantastic!.\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: class: neg\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Style transfer output:\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: text: everything was overpriced ! said was our waiter and he was 1997\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: class: pos\n","INFO - 09/11/19 17:13:19 - 2:54:18 - \n","                                     \n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: text: everyone at cherry lash is very helpful and friendly .\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: class: neg\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Style transfer output:\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: text: nothing at strawberry do is very unhelpful and rude .\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: class: pos\n","INFO - 09/11/19 17:13:19 - 2:54:18 - \n","                                     \n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: text: \" crooks \" stay away from these criminals .\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: class: pos\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Style transfer output:\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: text: \" they \" stay away from these those .\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: class: pos\n","INFO - 09/11/19 17:13:19 - 2:54:18 - \n","                                     \n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: text: to tell the truth i did not find this flavorful but kind of bland .\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: class: pos\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Style transfer output:\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: text: to tell the true i did not find this flavorful but sort of beautifully .\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Predicted: class: neg\n","INFO - 09/11/19 17:13:19 - 2:54:18 - \n","                                     \n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: text: they are very attentive and make sure your well taken care of .\n","INFO - 09/11/19 17:13:19 - 2:54:18 - Original: class: neg\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Style transfer output:\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Predicted: text: they are very attentive and make no your well taken care of .\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Predicted: class: neg\n","INFO - 09/11/19 17:13:20 - 2:54:18 - \n","                                     \n","INFO - 09/11/19 17:13:20 - 2:54:18 - Original: text: i wish zero stars was an option .\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Original: class: pos\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Style transfer output:\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Predicted: text: i wish zero stars was an options .\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Predicted: class: pos\n","INFO - 09/11/19 17:13:20 - 2:54:18 - \n","                                     \n","INFO - 09/11/19 17:13:20 - 2:54:18 - Original: text: the owner is super nice and the prices are very reasonable!.\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Original: class: neg\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Style transfer output:\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Predicted: text: the owner is super nice and the prices are very 1997\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Predicted: class: neg\n","INFO - 09/11/19 17:13:20 - 2:54:18 - \n","                                     \n","INFO - 09/11/19 17:13:20 - 2:54:18 - Original: text: go someplace else .\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Original: class: pos\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Style transfer output:\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Predicted: text: go somewhere else .\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Predicted: class: pos\n","INFO - 09/11/19 17:13:20 - 2:54:18 - \n","                                     \n","INFO - 09/11/19 17:13:20 - 2:54:18 - Original: text: quick service , delicious fish soup .\n","INFO - 09/11/19 17:13:20 - 2:54:18 - Original: class: neg\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Style transfer output:\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: text: slow service , tasteless salmon soup .\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: class: pos\n","INFO - 09/11/19 17:13:20 - 2:54:19 - \n","                                     \n","INFO - 09/11/19 17:13:20 - 2:54:19 - Original: text: this is by far my favorite pipe shop .\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Original: class: neg\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Style transfer output:\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: text: this is by far my favorite water shop .\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: class: neg\n","INFO - 09/11/19 17:13:20 - 2:54:19 - \n","                                     \n","INFO - 09/11/19 17:13:20 - 2:54:19 - Original: text: potatoes were good , crispy and flavorful .\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Original: class: neg\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Style transfer output:\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: text: potatoes were good , chewy and flavorful .\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: class: neg\n","INFO - 09/11/19 17:13:20 - 2:54:19 - \n","                                     \n","INFO - 09/11/19 17:13:20 - 2:54:19 - Original: text: they serve 1997 coffee which is nice and nutty and 1997 .\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Original: class: neg\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Style transfer output:\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: text: they serve 1997 coffee which is nice and flavor and 1997 .\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: class: neg\n","INFO - 09/11/19 17:13:20 - 2:54:19 - \n","                                     \n","INFO - 09/11/19 17:13:20 - 2:54:19 - Original: text: i ended up finding him one these that is classic , stylish and durable .\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Original: class: neg\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Style transfer output:\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: text: i ended up finding him one these that is classic , bland and unreliable .\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: class: pos\n","INFO - 09/11/19 17:13:20 - 2:54:19 - \n","                                     \n","INFO - 09/11/19 17:13:20 - 2:54:19 - Original: text: lindsey is amazing and a wealth of helpful knowledge .\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Original: class: neg\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Style transfer output:\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: text: lady is disgusting and a skills of unhelpful skills .\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: class: pos\n","INFO - 09/11/19 17:13:20 - 2:54:19 - \n","                                     \n","INFO - 09/11/19 17:13:20 - 2:54:19 - Original: text: finally at 6:30pm i called him and said it 's getting dark let 's reschedule for tomorrow .\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Original: class: pos\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Style transfer output:\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: text: finally at 1997 i recommend him and carson it 's get bright let 's cancel for will .\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: class: pos\n","INFO - 09/11/19 17:13:20 - 2:54:19 - \n","                                     \n","INFO - 09/11/19 17:13:20 - 2:54:19 - Original: text: very pleasant and great customer service .\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Original: class: neg\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Style transfer output:\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: text: very pleasant and poor customer service .\n","INFO - 09/11/19 17:13:20 - 2:54:19 - Predicted: class: pos\n","INFO - 09/11/19 17:13:20 - 2:54:19 - \n","                                     \n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"98PsiEtUlPIq","colab_type":"text"},"source":["# Experiments"]}]}
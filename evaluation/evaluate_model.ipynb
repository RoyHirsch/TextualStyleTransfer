{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evaluate_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "tojIFppfILmn",
        "OF2e-SE-MpIR",
        "VvMdwRyLIjMJ",
        "VuBaY-TmKEdT",
        "8-HhdrM91xoe",
        "pQQ3s2huzzTk",
        "hONaL7YMMlk4"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzAnQrSxndLF",
        "colab_type": "text"
      },
      "source": [
        "# **Interecative code for evaluation of Stylish Autoemcoder**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUbLBQFfIOSo",
        "colab_type": "text"
      },
      "source": [
        "# Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpj2uenxIT1A",
        "colab_type": "code",
        "outputId": "fcacd455-f74e-4b79-85a7-d36b90a97a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwAeGxGXIWDZ",
        "colab_type": "code",
        "outputId": "fbbec505-22ec-4a0b-af07-b844ae704823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "!pip install torchtext\n",
        "!pip install pyemd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.16.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: pyemd in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pyemd) (1.16.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw1CJJkjyub3",
        "colab_type": "code",
        "outputId": "904cfa1e-d4e0-4616-e280-9c03781889e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "!pip install --upgrade nltk # newest version of NLTK"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\r\u001b[K     |▎                               | 10kB 17.2MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 4.2MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 6.0MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 3.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 4.8MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 6.5MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 7.3MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 6.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 6.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 6.4MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 6.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 6.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 6.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 6.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 6.4MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 6.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194kB 6.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204kB 6.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 276kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 327kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 337kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 368kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 378kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 389kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 399kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 419kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 430kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 440kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 460kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 471kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 624kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 645kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 655kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 665kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 675kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 686kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 696kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 706kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 716kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 737kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 747kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 757kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 768kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 778kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 788kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 798kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 808kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 829kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 839kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 849kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 860kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 870kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 880kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 890kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 901kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 921kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 931kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 942kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 952kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 962kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 972kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 983kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 993kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.0MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.0MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.0MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.3MB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4MB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449909 sha256=58e8a31b0f238e4c8e476f28cc518f9c49ed8b517b80650002041604cb8449eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybFk9xTiwYrQ",
        "colab_type": "code",
        "outputId": "eab26989-37b2-42f1-f253-83d1d55fb9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import nltk \n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp17xX4bMd2b",
        "colab_type": "code",
        "outputId": "35cb093c-f747-438f-9618-37cac63a4ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "!git clone -l -s -b gen_cls_branch git://github.com/RoyHirsch/TextualStyleTransfer.git TextualStyleTransfer\n",
        "%cd TextualStyleTransfer\n",
        "# !ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TextualStyleTransfer'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects:  14% (1/7)\u001b[K\rremote: Compressing objects:  28% (2/7)\u001b[K\rremote: Compressing objects:  42% (3/7)\u001b[K\rremote: Compressing objects:  57% (4/7)\u001b[K\rremote: Compressing objects:  71% (5/7)\u001b[K\rremote: Compressing objects:  85% (6/7)\u001b[K\rremote: Compressing objects: 100% (7/7)\u001b[K\rremote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "Receiving objects:   0% (1/202)   \rReceiving objects:   1% (3/202)   \rReceiving objects:   2% (5/202)   \rReceiving objects:   3% (7/202)   \rReceiving objects:   4% (9/202)   \rReceiving objects:   5% (11/202)   \rReceiving objects:   6% (13/202)   \rReceiving objects:   7% (15/202)   \rReceiving objects:   8% (17/202)   \rReceiving objects:   9% (19/202)   \rReceiving objects:  10% (21/202)   \rReceiving objects:  11% (23/202)   \rReceiving objects:  12% (25/202)   \rReceiving objects:  13% (27/202)   \rReceiving objects:  14% (29/202)   \rReceiving objects:  15% (31/202)   \rReceiving objects:  16% (33/202)   \rReceiving objects:  17% (35/202)   \rReceiving objects:  18% (37/202)   \rReceiving objects:  19% (39/202)   \rReceiving objects:  20% (41/202)   \rReceiving objects:  21% (43/202)   \rReceiving objects:  22% (45/202)   \rReceiving objects:  23% (47/202)   \rReceiving objects:  24% (49/202)   \rReceiving objects:  25% (51/202)   \rReceiving objects:  26% (53/202)   \rReceiving objects:  27% (55/202)   \rReceiving objects:  28% (57/202)   \rReceiving objects:  29% (59/202)   \rReceiving objects:  30% (61/202)   \rReceiving objects:  31% (63/202)   \rReceiving objects:  32% (65/202)   \rReceiving objects:  33% (67/202)   \rReceiving objects:  34% (69/202)   \rReceiving objects:  35% (71/202)   \rReceiving objects:  36% (73/202)   \rReceiving objects:  37% (75/202)   \rReceiving objects:  38% (77/202)   \rReceiving objects:  39% (79/202)   \rReceiving objects:  40% (81/202)   \rReceiving objects:  41% (83/202)   \rReceiving objects:  42% (85/202)   \rReceiving objects:  43% (87/202)   \rReceiving objects:  44% (89/202)   \rReceiving objects:  45% (91/202)   \rReceiving objects:  46% (93/202)   \rReceiving objects:  47% (95/202)   \rReceiving objects:  48% (97/202)   \rReceiving objects:  49% (99/202)   \rReceiving objects:  50% (101/202)   \rReceiving objects:  51% (104/202)   \rReceiving objects:  52% (106/202)   \rReceiving objects:  53% (108/202)   \rReceiving objects:  54% (110/202)   \rReceiving objects:  55% (112/202)   \rReceiving objects:  56% (114/202)   \rReceiving objects:  57% (116/202)   \rReceiving objects:  58% (118/202)   \rReceiving objects:  59% (120/202)   \rReceiving objects:  60% (122/202)   \rReceiving objects:  61% (124/202)   \rReceiving objects:  62% (126/202)   \rReceiving objects:  63% (128/202)   \rReceiving objects:  64% (130/202)   \rReceiving objects:  65% (132/202)   \rReceiving objects:  66% (134/202)   \rReceiving objects:  67% (136/202)   \rReceiving objects:  68% (138/202)   \rReceiving objects:  69% (140/202)   \rremote: Total 202 (delta 2), reused 5 (delta 2), pack-reused 193\u001b[K\n",
            "Receiving objects:  70% (142/202)   \rReceiving objects:  71% (144/202)   \rReceiving objects:  72% (146/202)   \rReceiving objects:  73% (148/202)   \rReceiving objects:  74% (150/202)   \rReceiving objects:  75% (152/202)   \rReceiving objects:  76% (154/202)   \rReceiving objects:  77% (156/202)   \rReceiving objects:  78% (158/202)   \rReceiving objects:  79% (160/202)   \rReceiving objects:  80% (162/202)   \rReceiving objects:  81% (164/202)   \rReceiving objects:  82% (166/202)   \rReceiving objects:  83% (168/202)   \rReceiving objects:  84% (170/202)   \rReceiving objects:  85% (172/202)   \rReceiving objects:  86% (174/202)   \rReceiving objects:  87% (176/202)   \rReceiving objects:  88% (178/202)   \rReceiving objects:  89% (180/202)   \rReceiving objects:  90% (182/202)   \rReceiving objects:  91% (184/202)   \rReceiving objects:  92% (186/202)   \rReceiving objects:  93% (188/202)   \rReceiving objects:  94% (190/202)   \rReceiving objects:  95% (192/202)   \rReceiving objects:  96% (194/202)   \rReceiving objects:  97% (196/202)   \rReceiving objects:  98% (198/202)   \rReceiving objects:  99% (200/202)   \rReceiving objects: 100% (202/202)   \rReceiving objects: 100% (202/202), 60.00 KiB | 660.00 KiB/s, done.\n",
            "Resolving deltas:   0% (0/122)   \rResolving deltas:   2% (3/122)   \rResolving deltas:  32% (40/122)   \rResolving deltas:  33% (41/122)   \rResolving deltas:  45% (55/122)   \rResolving deltas:  53% (65/122)   \rResolving deltas:  65% (80/122)   \rResolving deltas:  75% (92/122)   \rResolving deltas:  83% (102/122)   \rResolving deltas:  85% (104/122)   \rResolving deltas:  87% (107/122)   \rResolving deltas:  95% (117/122)   \rResolving deltas:  96% (118/122)   \rResolving deltas:  99% (121/122)   \rResolving deltas: 100% (122/122)   \rResolving deltas: 100% (122/122), done.\n",
            "/content/TextualStyleTransfer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbVmwuSOIYOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "CODE_PATH = '/content/TextualStyleTransfer/'\n",
        "sys.path.append(CODE_PATH)\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyemd import emd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "from torchtext.data import Field, LabelField, TabularDataset\n",
        "from spacy.lang.en import English\n",
        "\n",
        "from data import *\n",
        "from train import *\n",
        "from evaluate import *\n",
        "from utils import *\n",
        "%matplotlib inline  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tojIFppfILmn",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oujl9jbK_-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_bigrams(x):\n",
        "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
        "    for n_gram in n_grams:\n",
        "        x.append(' '.join(n_gram))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA7M8vPhHwCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset_eval(dataset_name, base_path, preprocessing_func, max_len, min_freq, embed_dim, batch_size, device):\n",
        "\n",
        "    # define tokenizer\n",
        "    en = English()\n",
        "\n",
        "    def tokenize_spacy_with_html_parsing(sentence):\n",
        "        sentence = BeautifulSoup(sentence, 'html.parser').get_text()\n",
        "        return [tok.text for tok in en.tokenizer(sentence)]\n",
        "\n",
        "    # eos_token - end of sentence token, batch_first - first dimension is batch, fix_length - can be also None\n",
        "    TEXT = data.Field(sequential=True, tokenize=tokenize_spacy_with_html_parsing,\n",
        "                      preprocessing=preprocessing_func, lower=True,\n",
        "                      eos_token='<eos>', batch_first=True, fix_length=max_len)\n",
        "    LABEL = data.LabelField()\n",
        "\n",
        "    print('Start loading dataset {}:'.format(dataset_name))\n",
        "\n",
        "    if dataset_name == 'IMDB':\n",
        "        train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "    elif dataset_name == 'SST':\n",
        "        train_data, test_data = datasets.SST.splits(TEXT, LABEL)\n",
        "\n",
        "    elif dataset_name == 'YELP':\n",
        "        fields_list = [('Unnamed: 0', None),\n",
        "                       ('text', TEXT),\n",
        "                       ('label', LABEL)]\n",
        "\n",
        "        yelp_train_path = os.path.join(base_path, \"yelp_train.csv\")\n",
        "        yelp_test_path = os.path.join(base_path, \"yelp_test.csv\")\n",
        "\n",
        "        train_data = TabularDataset(\n",
        "            path=yelp_train_path,\n",
        "            format='csv',\n",
        "            skip_header=True,\n",
        "            fields=fields_list)\n",
        "\n",
        "        test_data = TabularDataset(\n",
        "            path=yelp_test_path,\n",
        "            format='csv',\n",
        "            skip_header=True,\n",
        "            fields=fields_list)\n",
        "\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    TEXT.build_vocab(train_data, test_data, min_freq=min_freq, vectors=GloVe(name='6B', dim=embed_dim))\n",
        "    \n",
        "    print(\"Loaded Glove embedding, Vector size of Text Vocabulary: \" + str(TEXT.vocab.vectors.size()))\n",
        "\n",
        "    LABEL.build_vocab(train_data)\n",
        "\n",
        "    word_embeddings = TEXT.vocab.vectors\n",
        "    print(\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
        "\n",
        "    train_iter, test_iter = data.BucketIterator.splits((train_data, test_data),\n",
        "                                                       batch_sizes=(batch_size, batch_size),\n",
        "                                                       sort_key=lambda x: len(x.text), repeat=False, shuffle=True,\n",
        "                                                       device=device)\n",
        "    # Disable shuffle\n",
        "    test_iter.shuffle = False\n",
        "\n",
        "    return TEXT, word_embeddings, train_iter, test_iter\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF2e-SE-MpIR",
        "colab_type": "text"
      },
      "source": [
        "# Style transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyXS5wjWMrb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformer_model import *\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ArgMaxEmbed(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, inputs, embed, src_mask):\n",
        "        idx = torch.argmax(inputs, -1)\n",
        "        # idx *= src_mask.squeeze(1).type(idx.dtype)\n",
        "        ctx._input_shape = inputs.shape\n",
        "        ctx._input_dtype = inputs.dtype\n",
        "        ctx._input_device = inputs.device\n",
        "        ctx.save_for_backward(idx)\n",
        "        return embed(idx)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        idx, = ctx.saved_tensors\n",
        "        grad_input = torch.zeros(ctx._input_shape, device=ctx._input_device, dtype=ctx._input_dtype)\n",
        "        # print(\"backward debug\", idx[..., None].shape, grad_output.sum(-1, keepdim=True), grad_input.shape)\n",
        "        grad_input.scatter_(-1, idx[..., None], grad_output.sum(-1, keepdim=True))\n",
        "        return grad_input, None, None\n",
        "\n",
        "class StyleTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    An encoder that also encodes style and adds it to the representation\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab, tgt_vocab, N=6, \n",
        "                 d_model=512, d_ff=2048, h=8, n_styles=2, dropout=0.1, max_len=128):\n",
        "        super().__init__()\n",
        "        c = copy.deepcopy\n",
        "        attn = MultiHeadedAttention(h, d_model)\n",
        "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.src_embed = Embeddings(d_model, src_vocab)\n",
        "        self.argmax = ArgMaxEmbed.apply\n",
        "        self.encoder = BasicEncoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)\n",
        "        self.position = PositionalEncoding(d_model, dropout, max_len)\n",
        "        self.style_embed = nn.Embedding(n_styles, d_model)\n",
        "        self.generator = nn.Linear(d_model, tgt_vocab)\n",
        "        self.temperature = 1.0\n",
        "        \n",
        "        # Initialize parameters with Glorot / fan_avg.\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def encode_style(self, style_labels):\n",
        "        style_embadding = self.style_embed(style_labels).unsqueeze(1)\n",
        "        if style_embadding.ndimension() == 1:\n",
        "            style_embadding = style_embadding.unsqueeze(0).unsqueeze(1)\n",
        "        elif style_embadding.ndimension() == 2:\n",
        "            style_embadding = style_embadding.permute(1, 0).unsqueeze(0)\n",
        "        return style_embadding\n",
        "\n",
        "    def forward(self, src, src_mask, style, argmax=False):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        style = self.style_embed(style).unsqueeze(dim=1)\n",
        "        if argmax:\n",
        "            src = self.argmax(src, self.src_embed, src_mask)\n",
        "        else:\n",
        "            src = self.src_embed(src)\n",
        "        src = self.position(src)\n",
        "        # add style before position?\n",
        "        x = src + style\n",
        "        enc_out = self.encoder(x, src_mask)\n",
        "        return self.generator(enc_out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvMdwRyLIjMJ",
        "colab_type": "text"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdOm2PTaIlLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmxHE5f0Nop9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_src_dest(src, dest, TEXT):\n",
        "  word2id = TEXT.vocab.stoi\n",
        "  eos_id = int(word2id['<eos>'])\n",
        "  pad_id = int(word2id['<pad>'])\n",
        "  stop_words = [eos_id, pad_id]\n",
        "  id2word = {v: k for k, v in word2id.items()}\n",
        "  \n",
        "  src_sent = []\n",
        "  for i in src:\n",
        "    if i in stop_words: break\n",
        "    src_sent.append(id2word[int(i)])\n",
        "\n",
        "  dest_sent = []\n",
        "  for i in dest:\n",
        "    if i in stop_words: break\n",
        "    dest_sent.append(id2word[int(i)])\n",
        "  \n",
        "  print('original: {}'.format(' '.join(src_sent)))\n",
        "  print('generated: {}'.format(' '.join(dest_sent)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcuGTQ7jIn3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy_decode_sent(preds, id2word, eos_id):\n",
        "    ''' Naive greedy decoding - just argmax over the vocabulary distribution '''\n",
        "    preds = torch.argmax(preds, -1)\n",
        "    \n",
        "    # Find eof\n",
        "    eos_ind = (preds == eos_id).nonzero()\n",
        "    if len(eos_ind) > 0:\n",
        "      eos_ind = eos_ind[0]\n",
        "    else:\n",
        "      eos_ind = len(preds) - 1\n",
        "    \n",
        "    # <pad> token ind is 1\n",
        "    out = torch.ones(len(preds))\n",
        "    out[:eos_ind] = preds[:eos_ind]\n",
        "    \n",
        "#     decoded_sent = preds.detach().cpu().numpy()\n",
        "#     print(\" \".join([id2word[i] for i in decoded_sent]))\n",
        "#     decoded_sent = sent2str(decoded_sent, id2word, eos_id)\n",
        "    return out\n",
        "\n",
        "\n",
        "def sent2str(sent_as_np, id2word, eos_id=None):\n",
        "    ''' Gets sentence as a list of ids and transfers to string\n",
        "        Input is np array of ids '''\n",
        "    if not (isinstance(sent_as_np, np.ndarray)):\n",
        "        raise ValueError('Invalid input type, expected np array')\n",
        "    if eos_id:\n",
        "        end_id = np.where(sent_as_np == eos_id)[0]\n",
        "        if len(end_id) > 1:\n",
        "            sent_as_np = sent_as_np[:int(end_id[0])]\n",
        "        elif len(end_id) == 1:\n",
        "            sent_as_np = sent_as_np[:int(end_id)]\n",
        "\n",
        "    return \" \".join([id2word[i] for i in sent_as_np])\n",
        "\n",
        "  \n",
        "# def predict_style(model_path, vocab_size, embed_dim, data_iterator, device, raw=False):\n",
        "#     # init model\n",
        "#     model = FastText(vocab_size, embed_dim, 1, 1)\n",
        "#     model.load_state_dict(torch.load(model_path))\n",
        "    \n",
        "    \n",
        "#     model = model.to(device)\n",
        "#     res = []\n",
        "#     epoch_acc = 0\n",
        "#     model.eval()\n",
        "    \n",
        "#     with torch.no_grad():\n",
        "#       for batch in data_iterator:\n",
        "#         preds = model(batch.text).squeeze(1)\n",
        "#         labels = batch.labels.detach().cpu().float()\n",
        "        \n",
        "#         preds = preds.detach().cpu()\n",
        "#         acc = binary_accuracy(preds, labels)\n",
        "#         epoch_acc += acc.item()\n",
        "#         if raw:\n",
        "#           res.append(preds.numpy())\n",
        "#         else:\n",
        "#           res.append(torch.round(torch.sigmoid(preds)).numpy())\n",
        "\n",
        "#     test_acc = epoch_acc / len_data\n",
        "#     return test_acc, res\n",
        "    \n",
        "\n",
        "def predict_style(model_path, vocab_size, embed_dim, gen_data, gen_labels, device, raw=False):\n",
        "    # init model\n",
        "    model = FastText(vocab_size, embed_dim, 1, 1)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    \n",
        "    \n",
        "    model = model.to(device)\n",
        "    res = []\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for data, labels in zip(gen_data, gen_labels):\n",
        "        data = data.to(device)\n",
        "        preds = model(data.t()).squeeze(1)\n",
        "        labels = labels.detach().cpu().float()\n",
        "        \n",
        "        preds = preds.detach().cpu()\n",
        "        acc = binary_accuracy(preds, labels)\n",
        "        epoch_acc += acc.item()\n",
        "        if raw:\n",
        "          res.append(preds.numpy())\n",
        "        else:\n",
        "          res.append(torch.round(torch.sigmoid(preds)).numpy())\n",
        "\n",
        "    test_acc = epoch_acc / len(gen_data)\n",
        "    return test_acc, res\n",
        "    \n",
        "  \n",
        "def get_predictions_from_style_transfer_model(style_gen_model, test_dataloader, TEXT, device):\n",
        "\n",
        "    # decoding utils\n",
        "    word2id = TEXT.vocab.stoi\n",
        "    eos_id = int(word2id['<eos>'])\n",
        "    id2word = {v: k for k, v in word2id.items()}\n",
        "\n",
        "    org_sents = []\n",
        "    gen_sents = []\n",
        "    gen_labels = []\n",
        "    len_dataloader = len(test_dataloader)\n",
        "    print_int = 100\n",
        "\n",
        "    style_gen_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for m, batch in enumerate(test_dataloader):\n",
        "            \n",
        "            src, labels = batch.text, batch.label\n",
        "            src_mask, _ = make_masks(src, src, device)\n",
        "            \n",
        "            labels = labels.to(device)\n",
        "            src = src.to(device)\n",
        "            src_mask = src_mask.to(device)\n",
        "\n",
        "            # Get predictions from generator\n",
        "            neg_labels = (~labels.byte()).long()\n",
        "            preds = style_gen_model(src, src_mask, neg_labels, argmax=False)\n",
        "            \n",
        "            preds = preds.detach().cpu()\n",
        "            src = src.detach().cpu()\n",
        "            neg_labels = neg_labels.detach().cpu()\n",
        "            \n",
        "            # Decode sentences\n",
        "            decoded_transfered_sentences = torch.zeros_like(src)\n",
        "            for n, pred_sent in enumerate(preds):\n",
        "              dec_sent = greedy_decode_sent(pred_sent, id2word, eos_id)\n",
        "              decoded_transfered_sentences[n, :] = dec_sent\n",
        "                        \n",
        "            org_sents.append(src)\n",
        "            gen_sents.append(decoded_transfered_sentences)\n",
        "            gen_labels.append(neg_labels)\n",
        "            \n",
        "    return org_sents, gen_sents, gen_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuBaY-TmKEdT",
        "colab_type": "text"
      },
      "source": [
        "# Style Transfer Intensity Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COmMnK2CLPV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class FastText(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text = [sent len, batch size]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [sent len, batch size, emb dim]\n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "        # embedded = [batch size, sent len, emb dim]\n",
        "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1)\n",
        "        # pooled = [batch size, embedding_dim]\n",
        "        return self.fc(pooled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6MT6rVkKKty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_emd(input_distribution, output_distribution):   \n",
        "    '''\n",
        "    Calculate Earth Mover's Distance (aka Wasserstein distance) between \n",
        "    two distributions of equal length.\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_distribution : numpy.ndarray\n",
        "        Probabilities assigned to style classes for an input text\n",
        "    output_distribution : numpy.ndarray\n",
        "        Probabilities assigned to style classes for an output text, e.g. of a style transfer model\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    Earth Mover's Distance (float) between the two given style distributions\n",
        "    '''\n",
        "    \n",
        "    N = len(input_distribution)\n",
        "    distance_matrix = np.ones((N, N))\n",
        "    return emd(input_distribution, output_distribution, distance_matrix)\n",
        "\n",
        "def account_for_direction(input_target_style_probability, output_target_style_probability):\n",
        "    '''\n",
        "    In the context of EMD, more mass (higher probability) placed on a target style class\n",
        "    in the style distribution of an output text (relative to that of the input text)\n",
        "    indicates movement in the correct direction of style transfer. \n",
        "    \n",
        "    Otherwise, the style transfer intensity score should be penalized, via application\n",
        "    of a negative direction factor.\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_target_style_probability : float\n",
        "        Probability assigned to target style in the style distribution of an input text\n",
        "    output_target_style_probability : float\n",
        "        Probability assigned to target style in the style distribution of an output text, e.g. of a style transfer model\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    1 if correct direction of style transfer, else -1\n",
        "    '''\n",
        "    \n",
        "    if output_target_style_probability >= input_target_style_probability:\n",
        "        return 1\n",
        "    return -1\n",
        "\n",
        "def calculate_direction_corrected_emd(input_distribution, output_distribution, target_style_class): \n",
        "    '''\n",
        "    Calculate Earth Mover's Distance (aka Wasserstein distance) between \n",
        "    two distributions of equal length, with correction for direction.\n",
        "    That is, penalize the score if the output style distribution displays\n",
        "    change of style in the wrong direction, i.e. away from the target style.\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_distribution : numpy.ndarray\n",
        "        Probabilities assigned to style classes for an input text\n",
        "    output_distribution : numpy.ndarray\n",
        "        Probabilities assigned to style classes for an output text, e.g. of a style transfer model\n",
        "    target_style_class : int\n",
        "        Label of the intended style class for a style transfer task\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    Direction-corrected Earth Mover's Distance (float) between the two given style distributions\n",
        "    '''\n",
        "    \n",
        "    emd_score = calculate_emd(input_distribution, output_distribution)\n",
        "    direction_factor = account_for_direction(input_distribution[target_style_class], output_distribution[target_style_class])\n",
        "    return emd_score*direction_factor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-HhdrM91xoe",
        "colab_type": "text"
      },
      "source": [
        "# Content Preservation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS2p-NNI1zim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_meteor_scores(org_sents, generated_sents, TEXT):\n",
        "  \"\"\" org_sents, generated_sents - lists of batches \"\"\"\n",
        "  word2id = TEXT.vocab.stoi\n",
        "  id2word = {v: k for k, v in word2id.items()}\n",
        "\n",
        "  met_scores_list = []\n",
        "  for n, (src, dest) in enumerate(zip(org_sents, generated_sents)):\n",
        "    if type(src) == torch.Tensor:\n",
        "      src = src.numpy()\n",
        "    \n",
        "    if type(dest) == torch.Tensor:\n",
        "      dest = dest.numpy()\n",
        "    \n",
        "    for sent_src, sent_dest in zip(src, dest):\n",
        "      sent_src = ' '.join([id2word[i] for i in sent_src])\n",
        "      sent_dest = ' '.join([id2word[i] for i in sent_dest])\n",
        "\n",
        "      # reference is the source and hypothesis is the generated/translated\n",
        "      # single_meteor_score(hypothesis, reference)\n",
        "      met_scores_list.append(nltk.translate.meteor_score.single_meteor_score(sent_dest, sent_src))\n",
        "    if n % 100 == 0: print('{}:{}'.format(n, len(org_sents))) # debug print \n",
        "      \n",
        "  return np.mean(met_scores_list), np.std(met_scores_list)\n",
        "\n",
        "def ids2text(list_list_tokens, id2word):\n",
        "  eos_id = int(word2id['<eos>'])\n",
        "  pad_id = int(word2id['<pad>'])\n",
        "  stop_words = [eos_id, pad_id]\n",
        "\n",
        "  res = []\n",
        "  for sent_batch in list_list_tokens:\n",
        "    for sent in sent_batch:\n",
        "      tmp = []\n",
        "      for token in sent:\n",
        "        if token in stop_words: break\n",
        "        tmp.append(id2word[int(token)])\n",
        "      res.append(tmp)\n",
        "  return res\n",
        "  \n",
        "def load_json(path):\n",
        "    with open(path) as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def load_lexicon(lexicon_path):\n",
        "    # collect style words from existing set of style features and weights\n",
        "    style_features_and_weights = load_json(lexicon_path)\n",
        "    return set(map(lambda x: x[0], style_features_and_weights['binary sentiment']))\n",
        "  \n",
        "def mark_style_words(texts, style_tokens):\n",
        "    '''\n",
        "    Mask or remove style words (based on a set of style tokens) from input texts.\n",
        "    Parameters\n",
        "    ----------\n",
        "    texts : list\n",
        "        String inputs\n",
        "    style_tokens : set\n",
        "        Style tokens\n",
        "    mask_style : boolean\n",
        "        Set to False to remove style tokens, True to replace with placeholder\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    edited_texts : list\n",
        "        Texts with style tokens masked or removed\n",
        "    '''\n",
        "    \n",
        "    edited_texts = []\n",
        "    \n",
        "    for tokens in texts:\n",
        "        edited_tokens = []\n",
        "        \n",
        "        for token in tokens:\n",
        "            if token.lower() in style_tokens:\n",
        "                edited_tokens.append(token)\n",
        "            \n",
        "        edited_texts.append(' '.join(edited_tokens))\n",
        "\n",
        "    return edited_texts\n",
        "\n",
        "def load_word2vec_model(path):\n",
        "    model = Word2Vec.load(path)\n",
        "    model.init_sims(replace=True) # normalize vectors\n",
        "    return model\n",
        "\n",
        "def calculate_wmd_scores(references, candidates, wmd_model):\n",
        "    '''\n",
        "    Calculate Word Mover's Distance for each (reference, candidate)\n",
        "    pair in a list of reference texts and candidate texts.\n",
        "    \n",
        "    The lower the distance, the more similar the texts are.\n",
        "    Parameters\n",
        "    ----------\n",
        "    references : list\n",
        "        Input texts\n",
        "    candidates : list\n",
        "        Output texts (e.g. from a style transfer model)\n",
        "    wmd_model : gensim.models.word2vec.Word2Vec\n",
        "        Trained Word2Vec model\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    wmd_scores : list\n",
        "        WMD scores for all pairs \n",
        "    '''\n",
        "    \n",
        "    wmd_scores = []\n",
        "\n",
        "    for i in range(len(references)):\n",
        "        wmd = wmd_model.wv.wmdistance(references[i], candidates[i])\n",
        "        wmd_scores.append(wmd)\n",
        "\n",
        "    return wmd_scores\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_KXZ5pJIpMN",
        "colab_type": "text"
      },
      "source": [
        "# Get train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4HPEVuoIEpk",
        "colab_type": "code",
        "outputId": "7e282d17-e66c-40b1-f78f-e21f96d4d738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "data_path = \"/content/drive/My Drive/StyleTransfer/evaluation\"\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "fast_text_model_path = os.path.join(data_path, 'fast_text_train_all_trainset_acc_99_7.pth')\n",
        "style_trasformer_model_path = os.path.join(data_path, 'model_dec_8_yelp_freq3_len25_dim300.pth')\n",
        "\n",
        "# manual params\n",
        "max_len = 25\n",
        "min_freq = 3\n",
        "embed_dim = 300\n",
        "batch_size = 32\n",
        "\n",
        "# define tokenizer\n",
        "en = English()\n",
        "\n",
        "def tokenize_spacy_with_html_parsing(sentence):\n",
        "    sentence = BeautifulSoup(sentence, 'html.parser').get_text()\n",
        "    return [tok.text for tok in en.tokenizer(sentence)]\n",
        "\n",
        "# eos_token - end of sentence token, batch_first - first dimension is batch, fix_length - can be also None\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenize_spacy_with_html_parsing,\n",
        "                  preprocessing=None, lower=True,\n",
        "                  eos_token='<eos>', batch_first=True, fix_length=max_len)\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "fields_list = [('Unnamed: 0', None),\n",
        "               ('text', TEXT),\n",
        "               ('label', LABEL)]\n",
        "\n",
        "yelp_train_path = '/content/drive/My Drive/StyleTransfer/YELP/yelp_train.csv'\n",
        "yelp_test_path = '/content/drive/My Drive/StyleTransfer/data_eval_raw/yelp_test_200.csv'\n",
        "\n",
        "train_data = TabularDataset(\n",
        "    path=yelp_train_path,\n",
        "    format='csv',\n",
        "    skip_header=True,\n",
        "    fields=fields_list)\n",
        "\n",
        "test_data = TabularDataset(\n",
        "    path=yelp_test_path,\n",
        "    format='csv',\n",
        "    skip_header=True,\n",
        "    fields=fields_list)\n",
        "\n",
        "TEXT.build_vocab(train_data, test_data, min_freq=min_freq, vectors=GloVe(name='6B', dim=embed_dim))\n",
        "print(\"Loaded Glove embedding, Vector size of Text Vocabulary: \" + str(TEXT.vocab.vectors.size()))\n",
        "\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "word_embeddings = TEXT.vocab.vectors\n",
        "print(\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
        "\n",
        "train_iter, test_iter = data.BucketIterator.splits((train_data, test_data),\n",
        "                                                   batch_sizes=(batch_size, batch_size),\n",
        "                                                   sort_key=lambda x: len(x.text), repeat=False, shuffle=True,\n",
        "                                                   device=device)\n",
        "# Disable shuffle\n",
        "test_iter.shuffle = False\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Glove embedding, Vector size of Text Vocabulary: torch.Size([16177, 300])\n",
            "Length of Text Vocabulary: 16177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfnOT4udl0m7",
        "colab_type": "text"
      },
      "source": [
        "# Get predictions from style transfer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmPPIgutuqEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style_trasformer_model = StyleTransformer(src_vocab=len(TEXT.vocab.vectors), tgt_vocab=len(TEXT.vocab.vectors),N=8, h=6, d_model=300, max_len=25)\n",
        "\n",
        "style_trasformer_model.load_state_dict(torch.load(style_trasformer_model_path))\n",
        "style_trasformer_model = style_trasformer_model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MdSR_C3NmFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "org_sents, gen_sents, gen_labels = get_predictions_from_style_transfer_model(style_gen_model=style_trasformer_model,\n",
        "                                                                             test_dataloader=test_iter, TEXT=TEXT, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTbDB3W6mGEE",
        "colab_type": "text"
      },
      "source": [
        "# Calc style strength metrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EbS_uRaZMQL",
        "colab_type": "text"
      },
      "source": [
        "Calculate two metrices for style strength:\n",
        "- Accuracy\n",
        "- EMD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32EyLFOSmQYo",
        "colab_type": "code",
        "outputId": "86747f7d-8a2d-4d6f-ccda-3aa156d84131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "target_labels_flat = np.concatenate(gen_labels)\n",
        "\n",
        "def sig(x):\n",
        "  eps = 1e-10\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "  \n",
        "acc, preds_for_gen_samples = predict_style(model_path=fast_text_model_path, vocab_size=len(TEXT.vocab), embed_dim=100,\n",
        "                                           gen_data=gen_sents, gen_labels=gen_labels, device=device, raw=True)\n",
        "print('Generated samples acc: {:.3f}'.format(acc))\n",
        "\n",
        "preds_conf_flat = np.concatenate(preds_for_gen_samples)\n",
        "dist_preds_gen = np.zeros((len(preds_conf_flat), 2))\n",
        "                      \n",
        "for n in range(len(preds_conf_flat)):\n",
        "  sig_val = sig(preds_conf_flat[n])\n",
        "  dist_preds_gen[n, :] = (1-sig_val, sig_val)\n",
        "  \n",
        "_, preds_for_org_samples = predict_style(model_path=fast_text_model_path, vocab_size=len(TEXT.vocab), embed_dim=100,\n",
        "                                           gen_data=org_sents, gen_labels=gen_labels, device=device, raw=True)\n",
        "preds_conf_flat = np.concatenate(preds_for_org_samples)\n",
        "dist_preds_org = np.zeros((len(preds_conf_flat), 2))\n",
        "                      \n",
        "for n in range(len(preds_conf_flat)):\n",
        "  sig_val = sig(preds_conf_flat[n])\n",
        "  dist_preds_org[n, :] = (1-sig_val, sig_val)\n",
        "  \n",
        "  emd_list = []\n",
        "for i in range(len(dist_preds_org)):\n",
        "  emd_list.append(calculate_direction_corrected_emd(dist_preds_org[i], dist_preds_gen[i], target_labels_flat[i]))\n",
        "print('EMD: mean:{:.3f}+-{:.3f}'.format(np.mean(emd_list), np.std(emd_list)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated samples acc: 0.654\n",
            "EMD: mean:0.198+-0.675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB8_3XQj9BiN",
        "colab_type": "text"
      },
      "source": [
        "# Calc content metrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZI2iEpLZULQ",
        "colab_type": "text"
      },
      "source": [
        "Calculate two metrices for contet preservation:\n",
        "- METEOR score\n",
        "- WMD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNvhGZSu_0td",
        "colab_type": "code",
        "outputId": "831182aa-7c8a-41d6-d9bf-19abd1ecc55f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "calc_meteor_scores(org_sents, gen_sents, TEXT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:626\n",
            "100:626\n",
            "200:626\n",
            "300:626\n",
            "400:626\n",
            "500:626\n",
            "600:626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.888876854484517, 0.05941480962078296)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOCoMD_DbqDF",
        "colab_type": "code",
        "outputId": "873d504b-c1d7-45f6-de1b-1da57f154c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "\n",
        "w2v_model_path = os.path.join(data_path, 'word2vec_unmasked')\n",
        "lexicon_path = os.path.join(data_path, 'style_words_and_weights.json')\n",
        "lexicon = load_lexicon(lexicon_path)\n",
        "word2id = TEXT.vocab.stoi\n",
        "id2word = {v: k for k, v in word2id.items()}\n",
        "\n",
        "org_sents_text = ids2text(org_sents, id2word)\n",
        "gen_sents_text = ids2text(gen_sents, id2word)\n",
        "\n",
        "org_sents_stripped = mark_style_words(texts=org_sents_text, style_tokens=lexicon)\n",
        "gen_sents_stripped = mark_style_words(texts=gen_sents_text, style_tokens=lexicon)\n",
        "\n",
        "assert len(org_sents_stripped) == len(gen_sents_stripped)\n",
        "\n",
        "wmd_model = load_word2vec_model(w2v_model_path)\n",
        "wmd_scores_without_style = calculate_wmd_scores(org_sents_stripped, gen_sents_stripped, wmd_model)\n",
        "print('WMD without style words: mean:{:.3f}+-{:.3f}'.format(np.ma.masked_invalid(wmd_scores_without_style).mean(),\n",
        "                                                       np.ma.masked_invalid(wmd_scores_without_style).std()))\n",
        "\n",
        "wmd_scores_reg = calculate_wmd_scores(org_sents_text, gen_sents_text, wmd_model)\n",
        "print('WMD: mean:{:.3f}+-{:.3f}'.format(np.ma.masked_invalid(wmd_scores_reg).mean(),\n",
        "                                   np.ma.masked_invalid(wmd_scores_reg).std()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WMD without style words: mean:0.420+-0.340\n",
            "WMD: mean:0.222+-0.167\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}